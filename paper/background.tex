\section{Background}

\vspace{2mm}
\noindent \textbf{Deep CNNs.} CNNs are a type of neural networks specialized for image data.
They exploit spatial locality of information in image pixels to construct a hierarchy of parametric feature extractors and transformers organized as layers of various types: \textit{convolutions}, which use image
filters from graphics, except with variable filter weights, to extract features; \textit{pooling}, which subsamples features in a spatial
locality-aware way; \textit{non-linearity} to apply a non-linear
function (e.g., ReLU) to all features; and \textit{fully connected},
which is a multi-layer perceptron.
A ``deep'' CNN just stacks such layers many times over.
Popular deep CNN model architectures include AlexNet \cite{alexnet}, VGG \cite{vggnet}, Inception, ResNet, SqueezeNet, and MobileNet.
In this work, the discussion and evaluation is focused on VGG, ResNet and SqueezeNet which are three widely used CNN models in real world use cases.
Nevertheless, our work is orthogonal to how CNNs are designed and the proposed approaches can be easily extended to any architecture.

\vspace{2mm}
\noindent \textbf{Deep CNN Explainability} With image classification models, natural question is if the model is truly identifying objects in the image or just using surrounding or other objects for making false predictions.
The various approaches used to explain CNN predictions can be broadly divided into two categories, namely gradient based and perturbation based approaches. Gradient based approaches generate a sensitivity map by computing the partial derivatives of model output with respect to every input pixel via back propagation.
In perturbation based approaches the output of the model is observed by masking out regions in the input image and there by identify the sensitive regions. The most popular perturbation based approach is occlusion experiments which was first introduced by Zeiler et. al. \cite{zeiler2014visualizing}.
Even though gradient approaches require only a single forward inference and a single backpropagation to generate the sensitivity map, the output may not be very intuitive and hard to understand because the salient pixels tend to spread over a very large are of the input image.
As a result in most real world use cases such as in medical imaging, practitioners tend to use occlusion experiments as the preferred approach for explanations as they produce high quality fine grained sensitivity maps despite being time consuming.

Over the years there has been several modifications proposed to the original occlusion experiment approach. More recently Zintgraf. et. al. \cite{zintgraf2017visualizing} proposed a variation to the original occlusion experiment approach named \textit{Prediction Difference Analysis}. In this method instead of masking with a grey or black patch, samples from surrounding regions in the image are chosen as occlusion patches.
In our work we mainly focus on the original occlusion experiment method. But, the methods and optimizations proposed in our work are readily applicable to more advanced occlusion based explainability approaches.