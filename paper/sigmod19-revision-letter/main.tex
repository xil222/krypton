\documentclass[preprint]{vldb}

\usepackage{booktabs} % For formal tables
\usepackage{amsmath}
\usepackage{graphicx,xspace,verbatim,comment}
\usepackage{hyperref,array,color,balance,multirow}
\usepackage{balance,float,url,amsfonts,alltt}
\usepackage{mathtools,rotating,amsmath,amssymb}
\usepackage{color,ifpdf,fancyvrb,array}
\usepackage{etoolbox,listings,subcaption}
\usepackage{bigstrut,morefloats}
\usepackage[boxruled]{algorithm2e}
\usepackage{pbox}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\newcommand{\eat}[1]{}
\newcommand{\red}{\textcolor{red}}
\newcommand{\system}{\textsc{Krypton}}

\pagenumbering{arabic}

\title{Revision Response Letter}

\author{}

\makeatletter
\def\@copyrightspace{\relax}
\makeatother

\begin{document}

\maketitle

We thank the reviewers for their feedback and suggestions.
We have made the utmost effort to carefully incorporate all of the feedback. 
We think the paper has improved as a result of these changes.
All changes made to the paper have been highlighted with red font color.
In this letter, we discuss the changes made and respond to the specific revision items listed in the reviews and metareview.

As an overall summary, we made following major changes to the paper, aligned with the metareviewer's list:
\begin{enumerate}

	\item Item (a): Expanded the discussion of the broader applicability of our ideas for other use cases in the new \textbf{Section 5.3} titled ``Summary and Discussion.''

	\item Item (b): Added an experiment in \textbf{Section 5.2} to evaluate the memory overhead of our incremental inference approach. Due to space constraints, the details are deferred to \textbf{Appendix F} and referenced in the body.

	\item Item (c): Expanded the discussion of our PyTorch integration in \textbf{Appendix B} and referenced it from the body (first para of \textbf{Section 5}).

	\item Item (d): Added a related work paragraph in \textbf{Section 6} on methods for accelerating CNN inference. We referenced EVA$^2$ method and explained how the optimizations proposed by our system are complementary to EVA$^2$. Please also see our response to R2 W1.

	\item Item (e): Added a related work paragraph in \textbf{Section 6} on CNN explanation methods, including the \textit{Axiomatic Attribution for Deep Networks} method. We also performed an empirical comparison between that method and OBE. Due to space constraints, the runtime plot and heatmap visualizations are deferred to \textbf{Appendix H} and referenced in the body.
	
	\item Item (f): Addressed all other reviewer comments, as detailed below.
	
\end{enumerate}


\section{Reviewer 1}

\vspace{2mm}
\noindent \textbf{W1:} \textit{Writing is very difficult to follow. In particular, the calculation of receptive field across layers (and its efficiency of calculation) is very hard to follow. I am not sure if there is an alternative notation that would be clearer.}

\vspace{2mm}
\noindent \textbf{Response:}\\
We have rewritten parts of the text in \textbf{Section 3.2} (on receptive field) and \textbf{Section 3.1} (on efficiency calculations) to explain them in a more intuitive way. We also reference the corresponding pictorial illustrations in Figure 4 where appropriate. As for the formal definitions and notation, we need them for technical precision lest we introduce ambiguity in explaining exactly how our rewrite framework works.

\vspace{2mm}
\noindent \textbf{W2:} \textit{Not clear to me OBE is the best choice and other interpretability methods are generally cheaper.}

\vspace{2mm}
\noindent \textbf{Response:} \\
In the practical literature on using CNNs, we found that OBE is often preferred over other methods by domain users, e.g., in radiology, since it produces high quality heatmaps and its process is intuitive for  non-technical users [10, 34]. There are also more recent versions of OBE such as ``Prediction Difference Analysis'' in ICLR 2017, which shows it is crucial to optimize OBE execution. We have added a paragraph in \textbf{Section 6} to summarize major CNN explanation approaches and cited the related work that highlight the importance of OBE for producing explanations. 

As a meta level point, we do not claim that OBE is the ``best choice,'' and we recognize CNN explanation methods are an active ML research topic. We leave it to future work to perform benchmark comparisons among such methods. Our goal in this paper is to optimize OBE from the data systems standpoint given its growing adoption among practitioners in important application settings. 

\vspace{2mm}
\noindent \textbf{D1:} \textit{Very little work in the interpretability space is cite. Most relevant is ``Axiomatic Attribution for Deep Networks'' which computes per-pixel attributions by comparing importance relative to a black pixel, similar to occlusion, but does so through mathematical analysis of the network, not many inferences. Comparing to this seems important to justify why OBE is better.}

\vspace{2mm}
\noindent \textbf{Response:} \\
We thank the reviewer for this reference. We have cited this paper and a few others from the relevant literature in a new paragraph \textbf{Section 6} summarizing CNN explanation methods. We also performed an empirical comparison of OBE against the method prescribed in ``Axiomatic Attribution of Deep Networks,'' which is called the Integrated Gradients (IGD) method. Due to space constraints, we have added these results and visual depictions of the heatmaps produced by both methods in \textbf{Appendix H}. We find that OBE seems to generate higher quality and better localized heatmaps compared to IGD. Of course, perceptual quality can vary in other cases and thus, IGD may be complementary to OBE. In terms of latency, the runtime of IGD is dependent on the number of gradient steps used for approximating the integration, which it expects the user to set.  The runtime of \system ~also depends on tunable parameters that are CNN-specific and dataset-specific. Our results show that the runtimes of IGD and OBE in \system ~are often comparable, with crossovers seen at different parameter regimes. Please also see our response to R1 W2. 

\vspace{2mm}
\noindent \textbf{D2:} \textit{I believe the components in this systems are individually interesting outside of interpretability. It would be interesting to understand if this ideas would be useful in other inference settings where inputs change only slightly between queries (for example, subsequent frames in a video).}

\vspace{2mm}
\noindent \textbf{Response:} \\
We thank the reviewer for this suggestion. We have added some discussion on this front in the new \textbf{Section 5.3} titled ``Summary and Discussion.'' We highlight some other use cases for applying and extending our optimization techniques. However, as the first paper on a comprehensive algebraic IVM and MQO framework for CNN inference, we prioritized technical depth and chose on OBE, a growing workload in practice, to show the efficiency gains possible with such ideas. That said, extending our optimizations to other use cases such as video analytics with minimal changes across frames may not be straightforward, since we would need to batch update patches of different sizes in unpredictable locations to exploit the throughput of the underlying hardware with our MQO idea. We leave a deeper treatment of such technical challenges to future work, some of which we are already exploring.


\section{Reviewer 2}

\vspace{2mm}
\noindent \textbf{W1:} \textit{The problem of occlusion-based explanations seems rather narrow, and it is not clear whether a more general tool could be used to get similar benefits here. For example, you could represent the occluding box moving around the image as a video, and then use techniques for fast inference on videos, for example the ones in the paper EVA: Exploiting Temporal Redundancy in Live Computer Vision (Buckler et al, ISCA 2018). Comparing against a more general tool would make this paper stronger.}

\vspace{2mm}
\noindent \textbf{Response:}\\
We thank the author for this reference. We have cited it and some other papers as part of a new paragraph on faster CNN inference in \textbf{Section 6}. We have summarized EVA$^2$ and some other systems and explained how \system ~is either complementary or orthogonal to these other systems.

EVA$^2$ exploits the temporal redundancy in video frames for faster inference. Conceptually, one can map OBE to a ``video'' consisting of the occluded images. However, unlike \system, EVA$^2$ cannot fully exploit spatial redundancy in potentially disparate patch locations. This is because video is a \textit{sequence} unlike the \textit{set} semantics we use for re-inference queries in our MQO technique. Furthermore, EVA$^2$ will still perform motion estimation computations on the entire frame for all frames. Nevertheless, at a meta level, the optimizations we propose in \system ~exist at the logical level and are applicable to virtually any hardware platform. In this sense, our techniques can be complementary to EVA$^2$, and we leave it to future work to extend our ideas to video analytics. Please also see our response to R1 D2.

On the optional request for an empirical comparison with EVA$^2$ as above, we realized that it requires their customized hardware-software co-designed stack for a fair comparison. Since it is not possible for us to replicate their hardware, we contacted the PC Chair to inquire if this empirical comparison was really necessary. In keeping with the PC Chair's response, we have skipped this empirical comparison and only include the qualitative discussion.

\vspace{2mm}
\noindent \textbf{W2:} \textit{Another weakness is that it seems likely that the techniques in Krypton can produce improvements for applications beyond the scope of occlusion-based explanations. For example, how would Krypton perform on an infrequently changing video feed, or one that changes in only a small part of the image? I think a comparison like this would improve the paper.}

\vspace{2mm}
\noindent \textbf{Response:}\\
Please see our response to Reviewer 1 D2.

\vspace{2mm}
\noindent \textbf{W3:} \textit{A third potential weakness lies in the approximate inference section, because there are many ways of doing approximate inference (e.g. low-precision computation, pruning, etc.) and it's not clear whether these could perform better than the new approximate inference methods proposed in this work. It would be an improvement to see some comparison to other methods for accelerating approximate inference.}

\vspace{2mm}
\noindent \textbf{Response:}\\
We have refined the introductory paragraph in \textbf{Section 4} to clarify the scope and qualitative nature of our approximate inference optimizations. Essentially, they exist at the logical level and are tied to the computational and semantic properties of OBE. Our ideas are complementary to more physical-level approximate optimizations such as low-precision computation and model pruning. That is, our techniques do not ``compete'' with these prior approximation techniques--rather, they can easily be used in addition for more efficiency gains. For instance, our  IVM framework can still exploit the spatial redundancy caused by OBE on a pruned CNN. Similarly, lower precision computations and associated compute hardware can readily be used underneath our techniques to speed up the low-level arithmetic.


\vspace{2mm}
\noindent \textbf{D1:} \textit{Figure 2 is hard to read. A lot is happening in that diagram for a "simplified illustration" and I think the font should be larger. Figure 15 in the appendix is very difficult to read. You should make the font size larger. In Figure 17 in the appendix, there seems to be an interesting phenomenon in which performance breaks down at a protective field threshold of 0.3 across all three images. This is interesting, and might be worth a sentence or two of discussion.}


\vspace{2mm}
\noindent \textbf{Response:}\\
We have refined the text in \textbf{Figure 2} and \textbf{Figure 15} and also increased the font size to make the figures easier to read. We have also added a sentence to explain the said phenomenon to the caption of \textbf{Figure 19} (old \textbf{Figure 17}).


\section{Reviewer 5}

\vspace{2mm}
\noindent \textbf{W1:} \textit{ The speedups achievable using the technique are dependent on the architectural properties of the CNN. The authors have explicitly identified the limitation in the paper. It would be good if given a CNN architecture, KRYPTON can provide an estimate of the speedup upfront to see if techniques in the paper will be beneficial for the architecture.}

\vspace{2mm}
\noindent \textbf{Response:}\\
We evaluate the theoretical bounds for attainable speedup by estimating FLOPs savings for CNN operations (\textbf{Section 2.2} Computational Cost of Inference).
We also estimate the speedup attainable from adaptive drill-down (\textbf{Section 4.2} Theoretical Speedups).
These speedups can be used to characterize the benefits based on architecture.
Calculating the total cost of inference is largely impractical as it requires calculating the computational cost, memory copy overheads etc.


\vspace{2mm}
\noindent \textbf{W2:} \textit{ Increasingly, CNNs are synthesized by learn to learn techniques. As future work, the authors should consider how a limited projective field can be included as a first class evaluation metric in such synthesis process and if this leads to new architectures that are IVM friendly.}

\vspace{2mm}
\noindent \textbf{Response:}\\
We identified this as one of the potential avenues to extend our work in the Summary and Discussion section (\textbf{Section 5.3}).
We agree with the reviewer and it is something we are currently looking into as an extension of our work.

\vspace{2mm}
\noindent \textbf{W3:} \textit{ The experimental evaluation is largely focused on the speed up obtained when dealing with one image (and its distortions) at a time. Maintaining the output tensors in memory incurs additional memory overhead. This can become significant in a shared serving environment where multiple inference requests (multiple raw images) are processed by the model concurrently. It would be good to have some experiment that also illustrates the memory overhead.}

\vspace{2mm}
\noindent \textbf{Response:}\\
We added an additional experiment to evaluate the memory usage behavior to the \textbf{Appendix F} and referred it from the main experiments section (\textbf{Section 5}).
The memory overhead of IVM approach will be much smaller (even up to $52\%$) than the full inference.
Krypton materializes a single copy of all CNN layers corresponding to the unmodified image and reuses it across a batch of occluded images with IVM. For IVM the size of required memory buffers are much smaller than the memory buffers required for full inference as only the updated patches need to be propagated.
% However, if incremental inference is performed with a batch size of one our system will incur more memory overheads as it now needs to allocate memory buffers to keep both a full copy of CNN features and incremental propagated patches.
% But when the batch size is greater than one it's memory overhead will be smaller.

\vspace{2mm}
\noindent \textbf{D2:} \textit{ Font size of text in Figure 2 is a bit small.}

\vspace{2mm}
\noindent \textbf{Response:}\\
We have revised the text in \textbf{Figure 2} and have increased the font size.

\vspace{2mm}
\noindent \textbf{D3:} \textit{I wonder if such IVM techniques can also be applied to non-CNN models that have certain structural properties. For instance, in case of multi-tower models, the perturbation of input values of one tower do not affect the intermediate tensors of other towers until the final few layers.}

\vspace{2mm}
\noindent \textbf{Response:}\\
The redundancies that exists in multi-tower models are very coarse-grained.
We can easily exploit these redundancies by simply caching the final layer of the towers that do not change.
On the other hand the focus of our system is on more fine-grained redundancies that exists in convolutional layers with small spatially localized updates.
However, in a multi-tower CNN model it is possible to integrate both approaches to exploit both coarse-grained and fine-grained redundancies.


\section{Reviewer 6}

\vspace{2mm}
\noindent \textbf{W1:} \textit{Occlusion-based inference is not such a common case, so the use case is small.}

\vspace{2mm}
\noindent \textbf{Response:}\\
Please see our response to Reviewer 1 W2.

\vspace{2mm}
\noindent \textbf{W2:} \textit{Not clear how this generalizes to some of the more complex architectures, such as DenseNet or ResNext -- could you please discuss this?}

\vspace{2mm}
\noindent \textbf{Response:}\\
We have addressed this in \textbf{Section 3.3} ``Extending to DAG like CNNs.'' The complexity of ResNeXt arises from element wise addition operations and the complexity of DenseNet arises from depth wise concatenation operations.

\vspace{2mm}
\noindent \textbf{W3:} \textit{Could there by other applications in the A/V space beyond partial occlusion that use the same techniques -- maybe painting parts of an image, or erasing parts of an image and them repainting it with something new?}

\vspace{2mm}
\noindent \textbf{Response:}\\
Please see our response to Reviewer 1 D2.

\vspace{2mm}
\noindent \textbf{W4:} \textit{There is a section on approximate inference, and it is rather ad-hoc, especially given the plethora of other methods such as quantization, pruning or collapsing a deep net, etc.)}

\vspace{2mm}
\noindent \textbf{Response:}\\
Please see our response to Reviewer 2 W3

\vspace{2mm}
\noindent \textbf{W5:} \textit{Not clear how Krypton is integrated into PyTorch. Is there a special API? What is the architecture of the integration?}

\vspace{2mm}
\noindent \textbf{Response:}\\
We have extended information about the integration of Krypton in to PyTorch in \textbf{Appendix B} and referred it from the experimental setup section (\textbf{Section 5}). Appendix \textbf{Figure 15} shows the architecture of the integration. Data is not transferred to the main memory in-between layers.

\vspace{2mm}
\noindent \textbf{D1:} \textit{I would have liked to see some network architectures where this does not work so well -- what are conditions where this is the case? Section 3 discusses this a bit, but could we have a clear characterization based on the architecture -- it seems that I can do a pre-calculations that based on the size of the occlusion I can calculate the savings?}

\vspace{2mm}
\noindent \textbf{Response:}\\
Yes. Given a CNN model architecture, in \textbf{Section 3.1} we provide an approach to estimate the expected speedups by applying our IVM method. Expected speedups is a function of the rate of projective field growth of the CNN which is determined by factors such as number of layers, convolution filter sizes, and filter stride values. For example, as shown in the End-to-End experimental results, VGG16 works really well as it uses small filter kernels and strides. But ResNet18 and Inception3 does not yield gains as high as VGG16.

\vspace{2mm}
\noindent \textbf{D2:} \textit{It is not clear to me how this is implemented. How do we bring the data back and forth to the GPU? Why are we getting such huge savings -- it is really just the number of add/multiplies saved? How do we now organize such irregular computations inside a GPU? Would be great to get answers to these questions.}

\vspace{2mm}
\noindent \textbf{Response:}\\
We explain the high level flow of our system in \textbf{Section 3.4 Algorithm 1}.
Though the updated regions are spatially localized they do not reside in contiguous memory locations. So before we perform the incremental CNN inference we copy the updated regions into a contiguous memory location.
Basically, we take a copy of the read-in context from the pre-materialized CNN layer values corresponding to the unmodified image and superimpose the updated patch.
We repeat this for all the CNN layers.

\textbf{Appendix B} summarizes how we have implemented this on PyTorch.
For the CPU environment we implement this purely on top of PyTorch using built-in tensor slicing and stitching primitives.
However, for the GPU environment such iterative memory copying operations introduce high overheads as the many GPU cores now have to idle wait for the slow memory copy operations. To overcome this we extended PyTorch by adding a custom GPU kernel which optimizes the input preparation for incremental inference by invoking parallel memory copy operations. This custom kernel is integrated to PyTorch using Python foreign function interface (FFI). Python FFI integrates with the Custom Kernel Interface layer which then invokes the Custom Memory Copy Kernel Implementation. The high-level architecture of the Custom Kernel integration is shown in \textbf{Figure 15}.

The savings mainly come from the saving of redundant computations. However, this introduces memory slicing and stitching overheads which our system tries to minimize using an optimized memory copy kernel. Once the initial input data is transferred to GPU memory, the entire incremental CNN inference can be done using only the GPU memory.

\vspace{2mm}
\noindent \textbf{D3:} \textit{Are there network architectures where these techniques work especially well or badly?}

\vspace{2mm}
\noindent \textbf{Response:}\\
Please see our response to Reviewer 6 D1.

\vspace{2mm}
\begin{sloppypar}
\noindent \textbf{D4:} \textit{There is work on self-adjusting computation, where a program learns how to react to changes in its inputs (\url{http://www.umut-acar.org/self-adjusting-computation}).
How does your work compare to this related work?}
\end{sloppypar}

\vspace{2mm}
\noindent \textbf{Response:}\\
Self-adjusting computation and other reactive computation methods found in the programming languages literature falls into the broader category of general incremental computation approaches. On the other hand the optimizations performed by our systems are specialized optimizations which takes into account the specific dataflow patterns in CNNs, the OBE workload, and also the characteristics of image datasets to perform incremental computations.
Therefore, the two lines of work are complementary to each other.

\vspace{2mm}
\noindent \textbf{D5:} \textit{Section 4.3 has a lot of formulas, but the reader is missing the intuition behind these formulas, Could they be made more accessible?}

\vspace{2mm}
\noindent \textbf{Response:}\\
The equations in Section 4.3 are essentially derivations from theoretical speedup calculation explained in \textbf{Section 4.2 Equation 28}.
To clarify this more we have refined the text in \textbf{Section 4.3}.

\end{document}
