%!TEX root = <main.tex>
\section{Background}

\vspace{2mm}
\noindent \textbf{Deep CNNs.} Deep CNNs are a type of neural networks specialized for image data.
They exploit spatial locality of information in image pixels to construct a hierarchy of parametric feature extractors and transformers organized as layers of various types: \textit{convolutions}, which use image
filters from graphics, except with variable filter weights, to extract features; \textit{pooling}, which subsamples features in a spatial
locality-aware way; \textit{batch-normalization}, which normalizes the output of the layer; \textit{non-linearity}, which applies a non-linear transformation (e.g., ReLU); \textit{fully connected}, which is the building block of a multi-layer perceptron; and \textit{softmax}, which emits predicted probabilities to each class label.
In most deep CNN architectures, the above layers are stacked together with one\'s output being fed as the input to the other.
Adding multiple layers element-wise or stacking multiple layers together depth-wise to produce a new layer is also present in some architectures.
Popular deep CNN model architectures include AlexNet \cite{alexnet}, VGG \cite{vggnet}, Inception~\cite{inception}, ResNet~\cite{resnet}, SqueezeNet~\cite{squeezenet}, and MobileNet~\cite{mobilenets}.
% In this work, the discussion and evaluation is focused on VGG-16 (16 layer version), ResNet-18 (18 layer version) and Inception-V3 (version 3) which are three widely used CNN models in real world transfer learning applications.
% Nevertheless, our work is orthogonal to the specifics of a particular architecture and the proposed approaches can be easily extended to any architecture.

\vspace{2mm}
\noindent \textbf{Deep CNN Explainability} \eat{With image recognition models, natural question is if the model is truly identifying objects in the image or just using surrounding or other objects for making false prediction.}
Various approaches used to explain CNN predictions can be broadly divided into two categories, gradient based and perturbation based approaches. Gradient based approaches generate a sensitivity heatmap by computing the partial derivatives of model output with respect to every input pixel via back propagation.
In perturbation based approaches the output of the model is observed by modifying regions on the input image and there by identifying the sensitive regions.
\eat{
Even though gradient approaches require only a single forward inference and a single backpropagation to generate the sensitivity heatmap, the output may not be very intuitive and hard to understand because the salient pixels tend to spread over a very large area of the input image.
}
Despite being time consuming, in most real world use cases such as in medical imaging, practitioners tend to use occlusion experiments \cite{zeiler2014visualizing}, a perturbation approach, as the preferred approach for explanations as they produce high quality fine grained sensitivity heatmaps ~\cite{jung2017deep} using a process which is very intuitive to the human observer ~\cite{miller2017explanation}.


\eat{
Occlusion experiment is one of the widely used perturbation based approaches which was first introduced by Zeiler et. al. \cite{zeiler2014visualizing}.

Also as explained in ~\cite{miller2017explanation}, the back-propagation based methods are based on the AI researchers\'~ intuition of what constitutes a ``good'' explanation.
But if the focus is on explaining decision to a human observer, then the approach used to produce the explanation should have a structure that humans accept.
}

\eat{
Over the years there has been several modifications proposed to the original occlusion experiment approach. More recently Zintgraf. et. al. \cite{zintgraf2017visualizing} proposed a variation to the original occlusion experiment approach named \textit{Prediction Difference Analysis}. In their method instead of masking with a black or gray patch, samples from surrounding regions in the image are chosen as occlusion patches.
In our work we mainly focus on the original occlusion experiment method. But, the methods and optimizations proposed in our work are readily applicable to more advanced occlusion based explainability approaches.
}