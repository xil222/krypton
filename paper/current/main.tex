%!TEX root = <main.tex>
\documentclass[10pt, sigconf]{acmart}
\usepackage[font=small,labelfont=bf]{caption}

\usepackage{graphicx,xspace,verbatim,comment}
\usepackage{hyperref,array,color,balance,multirow}
\usepackage{balance,float,url,amsfonts,alltt}
\usepackage{mathtools,rotating,amsmath,amssymb}
\usepackage{color,ifpdf,fancyvrb}
\usepackage{etoolbox,listings,subcaption}
\usepackage{bigstrut,morefloats,pbox}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{booktabs}
\usepackage{bm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\eat}[1]{}
\newcommand{\red}{\textcolor{red}}
\newcommand{\system}{\textsc{Krypton}}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother 

\newenvironment{packeditems}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\newenvironment{packedenums}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}


\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\DeclareMathOperator*{\argmin}{arg\,min}

% \setcopyright{none}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with 
\pagestyle{plain} % removes running headers

\begin{document}
\emergencystretch 1em

%\title{\textsc{Krypton}: Accelerating Occlusion based Deep CNN\\ Explainability Workloads}
\title{Incremental and Approximate Inference for\\Faster Occlusion-based Deep CNN Explanations}

%\author{Anonymous Author(s)}
% \affiliation{
  % \institution{University of California, San Diego}
% }
% \email{@eng.ucsd.edu}


\begin{abstract}
Deep Convolutional Neural Networks (CNNs) now match human accuracy in many image recognition tasks. This has lead to increasing adoption of deep CNNs in e-commerce, radiology, and other domains. Naturally, ``explaining'' CNN predictions is a key concern for many users. Since the internal working of CNNs are unintuitive for non-technical users, occlusion-based explanations are popular for determining which parts of an input image contribute the most to a given prediction. One occludes a region of the image using a patch and moves this patch across the image to yield a heatmap of changes to the prediction probability. Alas, this approach is computationally expensive due to the large number of re-inference requests produced, which could waste human time and raise resource costs. In this paper, we resolve this issue by casting occlusion-based CNN explanations as a new instance of the incremental view maintenance problem. We create a novel and comprehensive algebraic framework for incremental CNN inference that combines materialized views with multi-query optimization to avoid computational redundancy across re-inference requests. We then introduce two approximate inference optimizations that exploit the semantics of CNNs and the occlusion task to further reduce runtimes. We prototype our ideas in Python to create a tool we call \textsc{Krypton} that can support both CPUs and GPUs. Experiments with real data and CNNs show that \textsc{Krypton} reduces runtimes by up to 5x (14x) to produce exact (approximate) heatmaps without raising resource requirements.
\end{abstract}

\maketitle

\input{introduction}

% \input{background}

\input{system_overview}

\input{optimizer}

\input{experiments}

\section{Other Related Work}

\vspace{2mm}
\noindent \textbf{Incremental View Maintenance.} Incremental view maintenance \cite{chirkova2012materialized,gupta1995maintenance,levy1995answering} is a well studied concept in the relational data model context.
It is used in databases to support incremental updates to materialized views by applying differential updates without re-evaluating the complete view definitions.
The IVM approach presented in this paper falls into the broader category of \textit{``lazy maintenance''} strategy \cite{chirkova2012materialized}.
While there are many work in this context, most relevant to our work are \cite{nikolic2014linview} and \cite{zhao2017incremental}.
\cite{nikolic2014linview} extended the IVM concept to support IVM of linear algebra operators as opposed to classical database queries.
\cite{zhao2017incremental} extended the IVM concept to support multi-dimensional array data model with support for both relational style functions (e.g. filter and join) and also array style functions (e.g. smoothing and cross-matching).
The spatially localized transformations explained in Section 2.2 can be thought of as a form of \textit{``spatial array join''} explained in \cite{zhao2017incremental}.
However, the focus of \cite{zhao2017incremental} is on supporting efficient IVM on distributed sparse arrays by reducing the communication and chunk reassignment where as the focus of this paper is on formalizing the exact semantics of the IVM operators for spatially localized transformations in the emerging workload of occlusion based deep CNN predictions.
We also analyze the theoretical speedups that can be achieved for occlusion experiments using our IVM operator formulations.

\vspace{2mm}
\noindent \textbf{Multi Query Optimization.}


\section{Conclusions And Future Work}

\bibliographystyle{unsrt}
\bibliography{main}

\input{appendix}
\end{document}