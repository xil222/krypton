1em

Introduction
============

Deep Convolution Neural Networks (CNNs)
[@alexnet; @vggnet; @resnet; @inception] have revolutionized the
computer vision field with even surpassing human level accuracy in some
of the image recognition challenges such as ImageNet [@imagenet]. As a
result, there is wide adoption of deep CNN technology in a variety of
real world image recognition tasks in several domains including
healthcare [@kermany2018identifying; @islam2017abnormality], agriculture
[@mohanty2016using], security [@arbabzadah2016identifying], and
sociology [@wang2017deep]. Remarkably, United States Food and Drug
Administration Agency (US FDA) has already approved the use of deep CNN
based technologies for identifying diabetic retinopathy, an eye disease
found in adults with diabetes [@fdaretinopathy]. It is expected that
this kind of decision support systems will help the human radiologists
in fulfilling their workloads efficiently, such as operating as a
cross-checker for the manual decisions and also to prioritize potential
sever cases for manual inspection, and provide a remedy to the shortage
of qualified radiologists globally [@radiologistshortage].

However, despite their many success stories, one of the major criticisms
for deep CNNs, and deep neural networks in general is the black-box
nature of how they make predictions. In order to apply deep CNN based
techniques in critical applications such as health care, the decisions
should be explainable so that the practitioners can use their human
judgment to decide whether to rely on those predictions or not
[@jung2017deep].

![(a) Using CNNs for predicting Diabetic Retinopathy from OCT images.
(b) Occluding parts of the OCT image changes the predicted probability
for the disease. (c) By changing the position of the occlusion patch a
sensitivity heatmap is
produced.[]{data-label="fig:krypton_overview"}](./images/krypton_overview){width="\columnwidth"}

In order to improve the explainability of deep CNN predictions several
approaches have been proposed. One of the most widely used approach in
image recognition tasks is occlusion experiments
[@zeiler2014visualizing]. In occlusion experiments, as shown in
Figure \[fig:krypton\_overview\] (b), a square patch usually of black or
gray color is used to occlude parts of the image and record the change
in the predicted label probability. By changing the position of the
occlusion patch, usually by a small fixed number of pixels called
stride, a sensitivity heatmap for the predicted label can be generated
(similar to one shown in Figure \[fig:krypton\_overview\] (c)). If the
occlusion experiment is performed in interactive mode, the human
operator has the option of picking the occlusion patch positions by
marking a region on a visual interface. For example, if the scenario
shown in Figure \[fig:krypton\_overview\] is performed in interactive
mode, a human operator who understands OCT images will start evaluating
the image from the central region where she expects the pathological
region to be. In the non-interactive mode, which is also the most common
mode of performing occlusion experiments due to the high runtimes which
are not amenable for interactive performance, the heatmap values are
evaluated for all possible occlusion patch positions. Using this
heatmap, the regions in the image which are highly sensitive (or highly
contributing) to the predicted class can be identified (corresponds to
red color regions in the sensitivity heatmap shown in
Figure \[fig:krypton\_overview\] (c)). This localization of highly
sensitive regions then enable the practitioners to get an idea of the
the prediction process of the deep CNN.

However, occlusion experiments are highly compute intensive and time
consuming as each occlusion position has to be treated as a new image
and requires a separate CNN inference. In this work, our goal is to
apply database inspired optimizations to the occlusion based
explainability workload to reduce both the computational cost and the
runtime. This will also make occlusion experiments more amenable for
interactive diagnosis of CNN predictions. Our main motivation is based
on the observation that when performing CNN inference corresponding to
each individual patch position, there is a significant portion of
redundant computations which can be avoided. To avoid redundant
computations we introduce the notion of *incremental inference* of deep
CNN which is inspired by the incremental view maintenance technique
studied in the context of relational databases.

Due to the overlapping nature of how the Convolution kernel operates
(details to follow in Section \[sec:preliminaries\]), the size of the
modified patch will start growing as it progresses through more layers
in a CNN and the amount of redundant computations will reduce. However,
at deeper layers, the effect over the patch coordinates which are
radially further away from the center of the occlusion patch position
will be diminishing. Our second optimization is based on this
observation where we apply a form of *approximate inference* which
applies a threshold to limit the growth of the updating patch. By
applying propagation thresholds, a significant amount of computation
redundancy can be retained. We refer to this optimization as *projective
field thresholding*.

The third optimization is also a form of *approximate inference* which
is applicable only in the context of non-interactive mode. In most
occlusion experiment use cases, such as in medical imaging, the object
or pathological region of interest is contained in a relatively small
region of the image. In such situations, it is unnecessary to inspect
the original image at the same high resolution of striding the occluding
patch few pixels at a time, at all possible occlusion patch positions.
In this approach first, a low-resolution heatmap is generated using a
larger stride value with a relatively low computational cost. Only the
interesting regions will be then inspected further with a smaller stride
to produce a higher resolution output. In the interactive mode as the
human operator will be actively picking a set of occlusion patch
positions for the system to evaluate this optimization will not be
applicable. We refer to this optimization as *adaptive drill-down*.

Unlike the *incremental inference* approach which is exact, *projective
field thresholding* and *adaptive drill-down* are approximate
approaches. They essentially trade-off accuracy of the generated
sensitivity heatmap compared to the original, in favor of faster
execution. These changes in accuracy in the generated heatmap will be
visible all the way from quality differences which are almost
indistinguishable to the human eye to drastic structural differences,
depending on the level of approximation. This opens up an interesting
trade-off space of quality/accuracy versus runtime. [<span
style="font-variant:small-caps;">Krypton</span>]{}  provides user
configurable tuning parameters for easily picking an operational point
on this quality-runtime trade-off space.

Finally, we have implemented [<span
style="font-variant:small-caps;">Krypton</span>]{}  on top of PyTorch
deep learning toolkit by adding custom implementations for incremental
and approximate inference operations. It currently supports VGG16,
ResNet18, and InceptionV3 both on CPU and GPU environments, which are
three widely used deep CNN architectures. We evaluate our system on
three real-world datasets, 1) retinal optical coherence tomography
dataset (OCT), 2) chest X-Ray, and 3) more generic ImageNet dataset, and
show that [<span style="font-variant:small-caps;">Krypton</span>]{}  can
result in speedups over 10X. While we have implemented [<span
style="font-variant:small-caps;">Krypton</span>]{}  on top of PyTorch
toolkit, our work is largely orthogonal to the choice of the deep
learning toolkit; one could replace PyTorch with TensorFlow, Caffe2,
CNTK, MXNet, or implement from scratch using C/CUDA and still benefit
from our optimizations. Overall, this paper makes the following
contributions:

-   To the best of our knowledge, this is the first paper to study

**Outline.** The rest of this paper is organized as follows.

Background
==========

**Deep CNNs.** Deep CNNs are a type of neural networks specialized for
image data. They exploit spatial locality of information in image pixels
to construct a hierarchy of parametric feature extractors and
transformers organized as layers of various types: *Convolution*, which
use image filters from graphics, except with variable filter weights, to
extract features; *Pooling*, which subsamples features in a spatial
locality-aware way; *Batch-Normalization*, which normalizes the output
of the layer; *Non-Linearity*, which applies a non-linear transformation
(e.g., ReLU); *Fully Connected*, which is the building block of a
multi-layer perceptron; and *Softmax*, which emits predicted
probabilities to each class label. In most deep CNN architectures, the
above layers are stacked together with oneś output being fed as the
input to the other. Adding multiple layers element-wise or stacking
multiple layers together depth-wise to produce a new layer is also
present in some architectures. Popular deep CNN model architectures
include AlexNet [@alexnet], VGG [@vggnet], Inception [@inception],
ResNet [@resnet], SqueezeNet [@squeezenet], and MobileNet [@mobilenets].
**Deep CNN Explainability** Various approaches used to explain CNN
predictions can be broadly divided into two categories, gradient based
and perturbation based approaches. Gradient based approaches generate a
sensitivity heatmap by computing the partial derivatives of model output
with respect to every input pixel via back propagation. In perturbation
based approaches the output of the model is observed by modifying
regions on the input image and there by identifying the sensitive
regions. Despite being time consuming, in most real world use cases such
as in medical imaging, practitioners tend to use occlusion experiments
[@zeiler2014visualizing], a perturbation approach, as the preferred
approach for explanations as they produce high quality fine grained
sensitivity heatmaps  [@jung2017deep] using a process which is very
intuitive to the human observer  [@miller2017explanation].

Preliminaries and Overview {#sec:preliminaries}
==========================

In this section, we first formally state the problem and explain our
assumptions. Then we formalize the internals of critical layers in a
Deep CNN for the purpose of proposing our *incremental inference*
approach in Section 4.

Problem Statement and Assumptions
---------------------------------

\[table:preliminaries\_symbols\]

We are given a CNN $f$, an image $^{img}\mathcal{I}$ on which the
occlusion experiment needs to be run, the predicted class label $L$ for
$^{img}\mathcal{I}$, an occluding patch $\mathcal{P}$ in RGB format, and
occluding patch striding amount $^{\mathcal{P}}S$. In the interactive
case [<span style="font-variant:small-caps;">Krypton</span>]{}  expects
the user to also provide a set of interested occluding patch positions
$G$. In the non-interactive scenario [<span
style="font-variant:small-caps;">Krypton</span>]{}  uses the user
provided $^\mathcal{P}S$ value to initialize $G$ to the all possible
occluding positions. The occlusion experiment workload is to generate a
2-D heatmap $M$ with values corresponding to the coordinates in $G$
contain the predicted probability for $L$ by $f$ for the occluded image
$^{img}\mathcal{I}^{'}_{x,y}$ and zero otherwise. More precisely, we can
state the workload using the following set of logical statements:

$$\begin{aligned}
\label{eqn:mheight}
W_M =&~ \lfloor(\texttt{width}(^{img}\mathcal{I}) - \texttt{width}(\mathcal{P}) + 1)/^\mathcal{P}S\rfloor\\
\label{eqn:mwidth}
H_M =&~ \lfloor(\texttt{height}(^{img}\mathcal{I}) - \texttt{height}(\mathcal{P}) + 1)/^\mathcal{P}S\rfloor\\
M \in&~ \mathcal{\rm I\!R}^{H_M \times W_M}\\
\forall~ x,y \in&~ G:\\
\label{eqn:patchimpose}
& ^{img}\mathcal{I}^{'}_{x,y} \leftarrow {}^{img}\mathcal{I} ~ \bm\circ_{x,y} ~ \mathcal{P} \\
\label{eqn:outputval}
& M[x,y] \leftarrow f(^{img}\mathcal{I}^{'}_{x,y})[L]\end{aligned}$$

Step (\[eqn:mheight\]), and (\[eqn:mwidth\]) calculates the dimensions
of the generated heatmap $M$ which is dependent on the dimensions of
$^{img}\mathcal{I}$, $\mathcal{P}$, and $^\mathcal{P}S$. Step
(\[eqn:patchimpose\]) superimposes $\mathcal{P}$ on $^{img}\mathcal{I}$
with its top left corner placed on the (x,y) location of
$^{img}\mathcal{I}$. Step (\[eqn:outputval\]) calculates the output
value at the (x,y) location by performing CNN inference for
$^{img}\mathcal{I}^{'}_{x,y}$ using $f$ and taking the predicted
probability for the label $L$. Step \[eqn:patchimpose\] and
\[eqn:outputval\] are run for all occluding patch position values in
$G$. In the non-interactive case $G$ is initialized to
$G = [0, H_M) \times [0, W_M)$. In the interactive case it is possible
that human operator would provide multiple $G$s, one after the other,
for which the system has to evaluate iteratively.

We assume that $f$ is a CNN from a roster of well-known CNNs (currently,
VGG 16 layer version, ResNet 18 layer version, and Inception version 3).
This is a reasonable start since most recent CNN based image recognition
applications use only such well-known CNNs from model zoos
[@caffemodelzoo; @tfmodelzoo]. Nevertheless, our work is orthogonal to
the specifics of a particular architecture and the proposed approaches
can be easily extended to any architecture. We leave support for
arbitrary CNNs to future work.

Deep CNN Internals
------------------

Input and output of individual layers in a Deep CNN except for
Fully-Connected ones are arranged into three-dimensional volumes which
have a width, height, and depth. For example an RGB input image of
224$\times$224 spatial sizes can be considered as an input volume having
a width and height of 224 and a depth of 3 (corresponding to 3 color
channels). Every non Fully-Connected layer will take in an input volume
and transform it into another volume. A Fully-Connected layer takes in a
vector or flattened activation volume as input and transforms it into
another vector. For our purpose, these transformations can be broadly
divided into three categories based on how they operate spatially:

![image](images/cnn_simplified){width="\textwidth"}

-   Transformations that operate at the granularity of a global context.

    -   E.g. Fully-Connected

-   Transformations that operate at the granularity of individual
    spatial locations.

    -   E.g. ReLU, Batch Normalization

-   Transformations that operate at the granularity of a local spatial
    context.

    -   E.g. Convolution, Pooling

**Transformations that operate at the granularity of a global context.**
These transformations operate on a global context or in other words,
does not take into account the spatial information. Fully-Connected
layer, which is the only global context transformation in a CNN, takes
in an input vector and performs a vector-matrix multiplication with a
weight matrix and produces an output. As they perform one bulk
transformation there is no opportunity for exploiting redundancies. The
computational cost of a Full-Connected transformation is proportional to
the product of the size of the input and output vectors. Fully-Connected
layers are used as the last or last few layers in a CNN and only
accounts for a small fraction of the total computational cost.

**Transformations that operate at the granularity of individual spatial
locations.** These transformations essentially perform a $map(.)$
function on each individual activation value (see Figure
\[fig:cnn\_simplified\] (b)). Hence the output will have the same
dimensions as input. The computational cost incurred by these
transformations is proportional to the volume of the input (or output).
However, with incremental spatially localized updates in the input, such
as placing an occlusion patch, only the updated region is needed to be
recalculated. Extending these transformations to become change aware is
straightforward. The computational cost of the change aware incremental
transformation is proportional to the volume of the modified region.

**Transformations that operate at the granularity of a local spatial
context.** With incremental spatially localized updates in the input,
transformations that operate at the granularity of a local spatial
context also provide opportunities for exploiting redundancy and can be
made change aware. However, with local context transformations, such as
Convolution and Pooling, this extension is non-trivial due to the
overlapping nature of the spatial contexts. Each Convolution layer can
have $^lC_{\mathcal{O}}$ 3-D filter kernels organized into a 4-D array
$^l\mathcal{K}_{conv}$ with each having a smaller spatial width
$^lW_\mathcal{K}$ and height $^lH_\mathcal{K}$ compared to the width
$^lW_{\mathcal{I}}$ and height $^lH_{\mathcal{I}}$ of the input volume
$^l\mathcal{I}$, but has the same depth $^lC_{\mathcal{I}}$. During
inference, $c^{th}$ filter kernel is strided along the width and height
dimensions of the input and a 2-D activation map
$^cA=(^ca_{y,x})\in \mathcal{\rm I\!R}^{^lH_{\mathcal{O}} \times~ ^lW_{\mathcal{O}}}$
is produced by taking elementwise product between the kernel and the
input and adding a bias value as per Equation
\[eqn:elementwise\_product\]. The computational cost of each of these
individual element-wise product is proportional to the volume of the
filter kernel. Finally, these 2-D activation maps are stacked together
along the depth dimension to produce an output volume
$^{l}\mathcal{O} \in \mathcal{\rm I\!R}^{^lC_\mathcal{O} \times {}^lH_{\mathcal{O}} \times~ ^lW_{\mathcal{O}}}$
as per Equation \[eqn:conv\_operator\]. A simplified representation of
Convolution transformation is shown in Figure \[fig:cnn\_simplified\]
(a).

$$\begin{aligned}
\label{eqn:elementwise_product}
\begin{split}
^ca_{y,x} =& \sum_{k=0}^{^lC_\mathcal{I}} \sum_{j=0}^{^lH_\mathcal{K}-1} \sum_{i=0}^{^lW_\mathcal{K}-1} {}^l\mathcal{K}_{conv}[c, k, j, i] \\
& \quad \times {}^l\mathcal{I}[k,y-\floor{\frac{^lH_\mathcal{K}}{2}}+j,x-\floor{\frac{^lW_\mathcal{K}}{2}}+i] \\
& \quad + {}^l\mathcal{B}_{conv}[c]
\end{split}\end{aligned}$$

$$\begin{aligned}
\label{eqn:conv_operator}
\begin{split}
^l\mathcal{O} = [^0A, {}^1A, ... , {}^{^lC_\mathcal{O}-1}A]
\end{split}\end{aligned}$$

Pooling can also be thought as a Convolution operation with a fixed
(i.e. not learned) 2-D filter kernel $^l\mathcal{K}_{pool}$. But unlike
Convolution, Pooling operates independently on each depth slice of the
input volume. A Pooling layer takes a 3-D activation volume
$^l\mathcal{O}$ having a depth of $^lC$, width of $^lW_{\mathcal{I}}$,
and height of $^lH_{\mathcal{I}}$ as input and produces another 3-D
activation volume $^l\mathcal{O}$ which has the same depth of $^lC$,
width of $^lW_{\mathcal{O}}$, and height of $^lH_{\mathcal{O}}$ as the
output. Pooling kernel is generally strided with more than one pixel at
a time and hence $^lW_{\mathcal{O}}$ and $^lH_{\mathcal{O}}$ are
generally smaller than $^lW_{\mathcal{I}}$ and $^lH_{\mathcal{I}}$. A
simplified representation of Pooling transformation is shown in Figure
\[fig:cnn\_simplified\] (c).

**Relationship between Input and Output Spatial Sizes.** The output
volume’s spatial sizes $^lW_{\mathcal{O}}$ and $^lH_{\mathcal{O}}$ are
determined by the spatial sizes of the input volume $^lW_{\mathcal{I}}$
and $^lH_{\mathcal{I}}$, spatial sizes of the filter kernel
$^lW_\mathcal{K}$ and $^lH_\mathcal{K}$ and two other parameters:
**stride** $^lS$ and **padding** $^lP$. Stride is the amount of pixel
values used to slide the filter kernel at a time when producing a 2-D
activation map. It is possible to have two different values with one for
the width dimension $^lS_x$ and one for the height dimension $^lS_y$.
Generally $^lS_x \leq {}^lW_\mathcal{K}$ and
$^lS_y \leq {}^lH_\mathcal{K}$. In Figure \[fig:cnn\_simplified\]
Convolution transformation uses a stride value of 1 and Pool
transformation uses a stride value of 2 for both dimensions. Sometimes
in order to control the spatial size of the output activation map to be
same as the input activation map, one needs to pad the input feature map
with zeros around the spatial border. Padding $^lP$ captures the amount
of zeros that needs to be added. Similar to the stride $^lS$, it is
possible to have two separate values for padding with one for the width
dimension $^lP_x$ and one for the height dimension $^lP_y$. In Figure
\[fig:cnn\_simplified\] Convolution transformation pads the input with
one line of zeros from both dimensions. With these parameters defined,
the width (similarly height) of the output activation volume can be
defined as follows:

$$\begin{aligned}
\begin{split}
^lW_{\mathcal{O}} = (^lW_{\mathcal{I}} - {}^lW_\mathcal{K} + 1 + 2\times {}^lP_x)/^lS_x \\
\end{split}\end{aligned}$$

**Estimating the Computational Cost of Deep CNNs.** Deep CNNs are highly
compute intensive and out of the different types of layers Convolution
layers contribute to $90\%$ (or more) of the computation. Hence we can
estimate the computational cost of a Deep CNN by counting the number of
fused multiply-add (FMA) floating point operations (FLOPs) required by
Convolution layers for a single forward inference. For example, applying
a Convolution filter having the dimensions of ($^lC_\mathcal{I}$,
$^lH_{\mathcal{K}}$, $^lW_{\mathcal{K}}$) to a single spatial context
will require
$^lC_{\mathcal{I}} \times~ ^lH_{\mathcal{K}} \times~ ^lW_{\mathcal{K}}$
many FLOPs, each corresponding to a single element-wise multiplication.
Thus, the total amount of computations $^lQ$ required by that layer in
order to produce an output $\mathcal{O}$ having dimensions
$^lC_{\mathcal{O}} \times~ ^lH_{\mathcal{O}} \times~ ^lW_{\mathcal{O}}$,
and the total amount of computations $Q$ required to process the entire
set of convolution layers $L$ in the CNN can be calculated as per
Equation \[eqn:full\_local\] and  \[eqn:full\_all\]. However, in the
case of incremental updates only a smaller spatial patch having a width
$^lW_\mathcal{P}$ ($^lW_\mathcal{P}<={}^lW_{\mathcal{O}}$) and height
$^lH_\mathcal{P}$ ($^lH_\mathcal{P}<={}^lH_{\mathcal{O}}$) is needed to
be recomputed. The amount of computations required for the incremental
computation $^lQ_{inc}$ and total amount of incremental computations
$Q_{inc}$ required for the entire set of convolution layers $L$ will be
smaller than the above full computation values and can be calculated as
per Equation  \[eqn:inc\_local\] and  \[eqn:inc\_all\]. Notice that in
both of the above formulations computational cost calculation takes into
account the spatial sizes of the output or the output patch. The output
spatial sizes can be determined using the spatial sizes of the input and
other parameters as shown in the previous Section. In Section
\[sec:optimizer\] we show that the spatial sizes of the output patch can
also be predetermined using the input patch spatial sizes.

Based on the above quantities we define a new metric named *theoretical
speedup* R, which is the ratio between the total full computational cost
$Q$ and total incremental computation cost $Q_{inc}$ (see Equation
 \[eqn:redundancy\_ratio\]). This ratio essentially acts as a surrogate
for the theoretical upper-bound for computational and runtime savings
that can be achieved by applying incremental computations to deep CNNs.

$$\begin{aligned}
\label{eqn:full_local}
^lQ = (^lC_{\mathcal{I}} \times&~ ^lH_{\mathcal{K}} \times{}^lW_{\mathcal{K}}) \times~ (^lC_{\mathcal{O}} \times~ ^lH_{\mathcal{O}} \times{}^lW_{\mathcal{O}})\\
\label{eqn:full_all}
Q =&~ \sum_{l \in L}{}^lQ\\
\label{eqn:inc_local}
^lQ_{inc} = (^lC_{\mathcal{I}} \times&~ ^lH_{\mathcal{K}} \times~ ^lW_{\mathcal{K}}) \times~ (^lC_{\mathcal{O}} \times~ ^lH_{\mathcal{P}} \times~ ^lW_{\mathcal{P}})\\
\label{eqn:inc_all}
Q_{inc} =&~ \sum_{l \in L} ^lQ_{inc}\\
\label{eqn:redundancy_ratio}
R =&~ \frac{Q}{Q_{inc}}\end{aligned}$$

Optimizations {#sec:optimizer}
=============

In this section we explain *exact inference* and *approximate inference*
optimization in detail. In [<span
style="font-variant:small-caps;">Krypton</span>]{}  these optimizations
are applied on top of the currently dominant approach of performing CNN
inference on batches of images, with batch size selected to optimize the
hardware utilization, where each image corresponds to an occluded
instance of the original image. The batched inference is important as it
reduces per image inference time by amortizing the fixed overheads. In
our experiments we found that this simple optimization alone can give up
to  1.4X speedups on CPU environments and  2X speedups on the GPU
environment compared to the per image inference approach. Finally we
explain how [<span style="font-variant:small-caps;">Krypton</span>]{} 
configures its internal system configurations for *approximate
inference*.

Exact: Incremental Inference {#sec:inc_computation}
----------------------------

As explained earlier occlusion experiments in its naive form perform
many redundant computations. In order to avoid these redundancies layers
in a CNN have to be change aware and operate in an incremental manner
i.e. reuse previous computations as much as possible and compute only
the required ones. In this section, we focus on transformations that
operate at the granularity of a local spatial context (i.e. Convolution
and Pooling) as other types either have no redundancies (global context
transformations) or are trivial to make incremental aware (point
transformations). **Incremental Convolution and Pooling.**

\[table:optimizer\_symbols\]

In Section \[sec:preliminaries\] we explained that both Convolution and
Polling transformations can be cast into a form of applying a filter
along the spatial dimensions of the input volume. However, how each
transformation operates along the depth dimension is different. For our
purpose, we are only interested in finding the spatial propagation of
the patches in the input through the consecutive layers and hence both
these transformations can be treated similarly. The coordinates and the
dimensions (i.e. height and width) of the modified patch in the output
volume caused by a modified patch in the input volume are determined by
the coordinates and the dimensions of the input patch, sizes of the
filter kernel $^lH_\mathcal{K}$ and $^lW_\mathcal{K}$, padding values
$^lP_x$ and $^lP_y$, and the strides $^lS_x$ and $^lS_y$. For example
consider the simplified demonstration showing a cross-section of input
and output in Figure \[fig:dimensions\]. We use a coordinate system
whose origin is placed at the top left corner of the input. A patch
marked in red is placed on the input starting off at
$^lx^\mathcal{I}_\mathcal{P}, {}^ly^\mathcal{I}_\mathcal{P}$ coordinates
and has a height of $^lH^\mathcal{I}_\mathcal{P}$ and width of
$^lW^\mathcal{I}_\mathcal{P}$. The updated patch in the output starts
off at $^lx^\mathcal{O}_\mathcal{P}, {}^ly^\mathcal{O}_\mathcal{P}$ and
has a height of $^lH^\mathcal{O}_\mathcal{P}$ and width of
$^lW^\mathcal{O}_\mathcal{P}$. Note that due to the overlapping nature
of filter positions, to compute the output patch transformations have to
read a slightly larger context than the updated patch. This read in
context is shown by the blue shaded area in Figure \[fig:dimensions\].
The starting coordinates of this read-in patch are denoted by
$^lx^\mathcal{R}_\mathcal{P}, {}^ly^\mathcal{R}_\mathcal{P}$ and the
dimensions are denoted by
$^lW^\mathcal{R}_\mathcal{P}, {}^lH^\mathcal{R}_\mathcal{P}$. The
relationship between the coordinates and dimensions in the horizontal
axis (similarly in the vertical axis) can be expressed as follows:

![Simplified representation of input and output patch coordinates and
dimensions of Convolution and Pool
transformations.[]{data-label="fig:dimensions"}](images/dimensions){width="\columnwidth"}

$$\begin{aligned}
\label{eqn:xcoordinate}
^lx^\mathcal{O}_\mathcal{P} =&~ max\big(\lceil (^lP_x + {}^lx^\mathcal{I}_\mathcal{P} - {}^lW_\mathcal{K} + 1)/{}^lS_x \rceil, 0\big)\\
\label{eqn:patchwidth}
^lW^\mathcal{O}_\mathcal{P} =&~ min\big(\lceil (^lW^\mathcal{I}_\mathcal{P} +~ ^lW_\mathcal{K} - 1)/{}^lS_x \rceil, {}^lW_{\mathcal{O}}\big)\\
\label{eqn:xreadcoordinate}
^lx^\mathcal{R}_\mathcal{P} =&~ ^lx^\mathcal{O}_\mathcal{P} \times~ ^lS_x - {}^lP_x\\
\label{eqn:readpatchwidth}
^lW^\mathcal{R}_\mathcal{P} =&~ ^lW_\mathcal{K} + (^lW^\mathcal{O}_\mathcal{P}-1) \times {}^lS_x\end{aligned}$$

Equation \[eqn:xcoordinate\] calculates the starting coordinates of the
output patch. Use of padding effectively shifts the coordinate system
and therefore $^lP_x$ is added to correct it. Due to the overlapping
nature of filter kernels the affected span of the updated patch in the
input will be increased by $^lW_\mathcal{K}-1$ amount and hence needs to
be subtracted from the input coordinates $^lx^\mathcal{I}_\mathcal{P}$
(a filter of size $^lW_\mathcal{K}$ which is placed starting at
$^lx^\mathcal{I}_\mathcal{P} - {}^lW_\mathcal{K} + 1$ will see the new
change at $^lx^\mathcal{I}_\mathcal{P}$). Equation \[eqn:patchwidth\]
calculates the width of the output patches. Once the output patch
coordinate and width are calculated it is straightforward to calculate
the read-in patch coordinate as per Equation \[eqn:xreadcoordinate\] and
the width as per Equation \[eqn:readpatchwidth\].

\[alg:incinference\]

**Input:**\
$^lT$ : *Original Transformation*\
$^l\mathcal{I}$ : *Pre-materialized input from original image*\
$[^l\mathcal{P^I}_1,...,{}^{l}\mathcal{P^I}_n]$ : *Input patches*\
$[(^lx^\mathcal{I}_{\mathcal{P}_1},{}^ly^\mathcal{I}_{\mathcal{P}_1}),...,({}^lx^\mathcal{I}_{\mathcal{P}_n},{}^ly^\mathcal{I}_{\mathcal{P}_n})]$
: *Input patch coordinates*\
$^lW^\mathcal{I}_\mathcal{P},{}^lH^\mathcal{I}_\mathcal{P}$ : *Input
patch dimensions*

**Output:**\
$[^l\mathcal{P^O}_1,...,{}^{l}\mathcal{P^O}_n]$ : *Output patches*\
$[(^lx^\mathcal{O}_{\mathcal{P}_1},{}^ly^\mathcal{O}_{\mathcal{P}_1}),...,({}^lx^\mathcal{O}_{\mathcal{P}_n},{}^ly^\mathcal{O}_{\mathcal{P}_n})]$
: *Output patch coordinates*\
$^lW^\mathcal{O}_\mathcal{P},{}^lH^\mathcal{O}_\mathcal{P}$ : *Output
patch dimensions*

*Calculate*
$[(^lx^\mathcal{O}_{\mathcal{P}_1},{}^ly^\mathcal{O}_{\mathcal{P}_1}),...,({}^lx^\mathcal{O}_{\mathcal{P}_n},{}^ly^\mathcal{O}_{\mathcal{P}_n})]$
*Calculate*
($^lW^\mathcal{O}_\mathcal{P},{}^lH^\mathcal{O}_\mathcal{P}$)
*Calculate*
$[({}^lx^\mathcal{R}_{\mathcal{P}_1},{}^ly^\mathcal{R}_{\mathcal{P}_1}),...,({}^lx^\mathcal{R}_{\mathcal{P}_n},{}^ly^\mathcal{R}_{\mathcal{P}_n})]$
*Calculate*
($^lW^\mathcal{R}_\mathcal{P},{}^lH^\mathcal{R}_\mathcal{P}$)
*Initialize*
$\mathcal{R} \in \mathcal{\rm I\!R}^{\texttt{depth}({}^l\mathcal{I}) \times {}^lH^\mathcal{R}_\mathcal{P} \times {}^lW^\mathcal{R}_\mathcal{P}}$

\[alg:line:memcpy\_loop\]
$T_1 \gets {}^l\mathcal{I}[:,{}^lx^\mathcal{R}_{\mathcal{P}_i}:{}^lx^\mathcal{R}_{\mathcal{P}_i}+{}^lW^\mathcal{R}_\mathcal{P},{}^ly^\mathcal{R}_{\mathcal{P}_i}:{}^ly^\mathcal{R}_{\mathcal{P}_i}+{}^lH^\mathcal{R}_\mathcal{P}]$
$T_2 \gets T_1 \bm\circ_{({}^{l}x^\mathcal{I}_{\mathcal{P}_i}-{}^{l}x^\mathcal{R}_{\mathcal{P}_i}),({}^ly^\mathcal{I}_{\mathcal{P}_i}-{}^ly^l\mathcal{R}_{\mathcal{P}_i})} {}^l\mathcal{P}_i$
$R[i,:,:] \gets T_2$

$[{}^l\mathcal{P}^\mathcal{O}_1,...,{}^l\mathcal{P}^\mathcal{O}_n] \gets T(\mathcal{R})$
**return**
$[{}^l\mathcal{P}^\mathcal{O}_1,...,{}^l\mathcal{P}^\mathcal{O}_n]$,
$[({}^lx^\mathcal{O}_{\mathcal{P}_1},{}^ly^\mathcal{O}_{\mathcal{P}_1}),...,({}^lx^\mathcal{O}_{\mathcal{P}_n},{}^ly^\mathcal{O}_{\mathcal{P}_n})],$
(${}^lW^\mathcal{O}_\mathcal{P},{}^lH^\mathcal{O}_\mathcal{P}$)

With all the geometric mappings defined we now explain the complete
incremental inference approach for a single layer. Algorithm
\[alg:incinference\] presents it formally. The procedure takes in the
original transformation $^lT$ of the $l^{th}$ layer, pre-materialized
input for the layer corresponding to the original image, a batch of
updated patches which are 3D volumes of activation values and their
geometric properties as input. Notice that for the first layer all the
elements in the batch of updated patches will be identical as the same
occlusion patch (in RGB format) is used. However, at latter layers they
will be different as the output of the first layer will be different due
to different read-in contexts. First calculates the geometric properties
of the output and the read-in patches. A temporary input volume $R$ is
initialized to hold the input patches with their read-in contexts. The
<span style="font-variant:small-caps;">for</span> loop iteratively
populates $R$ with corresponding patches. Finally, ${}^lT$ is applied to
$R$ to compute the output patches. In a CNN which has multiple such
layers chained together, the outputs of the procedure are fed as input
again for the incremental inference of the next layer along with the
pre-materialized input from the original image. However, at a boundary
of local context transformation and a global context transformation,
such as in Convolution $\rightarrow$ Fully-Connected or Pool
$\rightarrow$ Fully-Connected, full updated output has to be created
instead of propagating only the updated patches. The high-level steps
taken by the end-to-end incremental inference approach for occlusion
experiments can be summarized as follows:

1.  Take in CNN $f$, image ${}^{img}\mathcal{I}$, predicted class label
    $L$, occlusion patch $\mathcal{P}$, stride ${}^{\mathcal{P}}S$ for
    the $\mathcal{P}$, and the set of occluding patch placement
    positions $G$ as input.

2.  Pre-materialize output of all the transformations in $f$ by feeding
    in ${}^{img}\mathcal{I}$.

3.  Prepare the occluded images (${}^{img}\mathcal{I}^{'}_{x,y}$ s)
    corresponding to all positions in $G$.

4.  For batches of ${}^{img}\mathcal{I}^{'}_{x,y}$ as the input invoke
    the transformations in $f$ in a chained manner and calculate the
    corresponding values of heatmap $M$.

    -   For local context transformation invoke .

    -   For local context transformation that feeds in a global context
        transformation additionally materialize the full updated output.

    -   For all others invoke the original transformation.

5.  Return M as the final output.

![Theoretical speedup for popular CNN architectures with *incremental
inference*.[]{data-label="fig:redundancy_ratio"}](images/redundancy_ratio){width="\columnwidth"}

**Theoretical Speedup with Incremental Inference.** We analyze the
theoretical speedup that can be achieved with the *incremental
inference* approach when a square occlusion patch is placed on the
center[^1] of the input image. Figure \[fig:redundancy\_ratio\] shows
the results. VGG16 model results in the maximum theoretical speedup and
DenseNet121 model has the lowest theoretical speedup. Most CNN
architectures result in a theoretical speedup between 2-3. The
theoretical speedup for a CNN with *incremental inference* is determined
by the characteristics of its architecture such as the number of layers,
the sizes of the filter kernels, and the filter stride values. For
example, VGG16 which uses small Convolution filter kernels and strides
incurs a very high computational cost (15 GFLOPs) for a single full
inference. However, as the change propagation rate with small filter
sizes and strides is small, significant computational savings and
runtime speedups can be achieved with incremental inference.

[TODO: MQO + IVM]{}

**CPU versus GPU implementation concerns.** Through our experiments we
found that even though a straight-forward implementation of *incremental
inference* approach as shown in Algorithm \[alg:incinference\] produces
expected speedups for the CPU environment it performs poorly on the GPU
environment. The for loop on the line \[alg:line:memcpy\_loop\] of
Algorithm \[alg:incinference\] is essentially preparing the input for
$T$ by copying values from one part of the memory to another
sequentially. This sequential operation becomes a bottleneck for the GPU
implementation as it cannot exploit the available parallelism of the GPU
efficiently. To overcome this we have extended PyTorch by adding a
custom kernel written in CUDA language which performs the input
preparation more efficiently by parallelly copying the memory regions
for all items in the batch and then invoke the CNN transformation.
[TODO: Refer to hardware works, e.g. Li Tseng, Lin, Swanson, Yannis on
hardware accelerators]{}

**Element-wise addition and depth-wise concatenation.** Element-wise
addition and depth-wise concatenation are two widely used linear algebra
operators in CNN. Element-wise addition operator requires the two input
volumes to have exact same dimensions and the depth-wise concatenation
requires them to have same height and width dimensions. Consider a
situation for these operators as shown in Figure \[fig:la\_operators\]
where the first input has incremental spatial update starting at
${}^lx^\mathcal{I}_{\mathcal{P}_1},{}^ly^\mathcal{I}_{\mathcal{P}_1}$
coordinates with dimensions of ${}^lH^\mathcal{I}_{\mathcal{P}_1}$ and
${}^lW^\mathcal{I}_{\mathcal{P}_1}$ and for the second input starting at
${}^lx^\mathcal{I}_{\mathcal{P}_2},{}^ly^\mathcal{I}_{\mathcal{P}_2}$
coordinates with dimensions of ${}^lH^\mathcal{I}_{\mathcal{P}_2}$ and
${}^lW^\mathcal{I}_{\mathcal{P}_2}$. In this case the coordinates and
dimensions of both output and read-in patches will be the same and
computing them is essentially finding the bounding box of the two input
patches. For the horizontal axis (similarly to vertical axis) it can be
expressed as follows:

![Input-output coordinate and dimension mapping for element-wise
addition and depth-wise
concatenation.[]{data-label="fig:la_operators"}](images/la_operators){width="\columnwidth"}

$$\begin{aligned}
\begin{split}
{}^lx^\mathcal{O}_P =&~ \texttt{min}({}^lx^\mathcal{I}_{\mathcal{P}_1},{}^lx^\mathcal{I}_{\mathcal{P}_2})\\
{}^lW^\mathcal{O}_\mathcal{P} =&~ \texttt{max}({}^lx^\mathcal{I}_{\mathcal{P}_1}+{}^lW^\mathcal{I}_{\mathcal{P}_1},{}^lx^\mathcal{I}_{\mathcal{P}_2}+{}^lW^\mathcal{I}_{\mathcal{P}_2}) -\texttt{min}({}^lx^\mathcal{I}_{\mathcal{P}_1},{}^lx^\mathcal{I}_{\mathcal{P}_2})
\end{split}\end{aligned}$$

Approximate: Projective Field Thresholding
------------------------------------------

Projective field [@le2017receptive; @basiccnnoperations] of a CNN neuron
is the local region (including the depth) of the output volume which is
connected to it. The term is borrowed from the Neuroscience field where
it is used to describe the spatiotemporal effects exerted by a retinal
cell on all of the outputs of the neuronal circuitry
[@de2011projective]. For our work, the notion of projective field is
useful as it determines the change propagation path for incremental
changes. The three types of CNN transformations affect the size of the
projective field differently. Point transformations do not change the
projective field size while global context transformations increase it
to the maximum. Transformations that operate on a local spatial context
increase it gradually. The amount of increase in a local context
transformation is determined by the filter size and stride parameters.
At every transformation, the size of the projective field will increase
linearly by the filter size and multiplicatively by the stride value.

![(a) One dimensional Convolution demonstrating projective field growth
(filter size = 2, stride = 1). (b) *Projective field thresholding* with
$\tau = 5/7$.[]{data-label="fig:pf_truncate"}](images/pf_truncate){width="\columnwidth"}

Because of the projective field growth, even though there will be many
computational redundancies in the early layers towards the latter layers
it will decrease or even have no redundancies. However, we empirically
found that the projective field growth can be restrained up to a certain
extent without significantly sacrificing the accuracy. For a more
intuitive understanding of why this would work consider the simplified
1-D Convolution example shown in Figure \[fig:pf\_truncate\] (a). In the
example, a single neuron is modified (marked in red) and a filter of
size three is applied with a stride of one repeatedly four times. Since
the filter size is three, each updated neuron will propagate the change
to three neurons in the next output layer causing the projective field
to grow linearly. The histogram at the end of the fourth layer shows the
number of unique paths that are available between each output neuron and
the originally updated neuron in the first layer. It can be seen that
this distribution resembles a Gaussian where many of the paths are
connected to the central region. The amount of change in the output
layer is determined by both the number of unique paths and also the
individual weights of the connections. It can be shown that the
distribution of change in the output layer will converge to a Gaussian
distribution provided certain conditions are met for the weight values
of the filter kernel (more details in Appendix X).

As most of the change will be concentrated on the center we introduce
the notion of a projective field threshold $\tau ~ (0 < \tau \leq 1)$
which will be used to restrict the growth of the projective field. It
determines the maximum size of the projective field as a fraction of the
size of the output. Figure \[fig:pf\_truncate\] (b) demonstrates the
application of projective field thresholding with a $\tau$ value of
$5/7$. From the histogram generated for the projective field
thresholding approach, we can expect that much of the final output
change is maintained by this approach.

In [<span style="font-variant:small-caps;">Krypton</span>]{},
*projective field thresholding* is implemented on top of *incremental
inference* approach by applying set of additional constraints on
input-output coordinate mappings. For the horizontal dimension
(similarly to vertical dimension) the new set of calculations can be
expressed as follows:

$$\begin{aligned}
\label{eqn:normal_width_calc}
{}^lW^\mathcal{O}_\mathcal{P} = &~ \texttt{min}\big(\lceil ({}^lW^\mathcal{I}_\mathcal{P} + {}^lW_\mathcal{K} - 1)/{}^lS_x \rceil, {}^lW^\mathcal{O}_\mathcal{P}\big)\\
\label{eqn:check_tau}
\text{If}~ {}^lW_\mathcal{P}^\mathcal{O} & > \texttt{round}(\tau \times {}^lW^\mathcal{O}):\\
\label{eqn:new_width_calc_with_tau}
& {}^lW^\mathcal{O}_\mathcal{P} = \texttt{round}(\tau \times {}^lW^\mathcal{O})\\
\label{eqn:new_in_width}
& {}^lW^\mathcal{I}_{\mathcal{P}_{new}} = {}^lW^\mathcal{O}_{\mathcal{P}} \times {}^lS_x - {}^lW_{\mathcal{K}} + 1\\
\label{eqn:new_x_coord}
& {}^lx^{\mathcal{I}}_\mathcal{P} \mathrel{+}= ({}^lW^\mathcal{I}_\mathcal{P} - {}^lW^\mathcal{I}_{\mathcal{P}_{new}})/2\\
\label{eqn:new_input_width}
& {}^lW^\mathcal{I}_{\mathcal{P}} = {}^lW^\mathcal{I}_{\mathcal{P}_{new}}\\
\label{eqn:new_output_x}
{}^lx^\mathcal{O}_\mathcal{P} = & \texttt{max}\big(\lceil ({}^lP_x + {}^lx^\mathcal{I}_\mathcal{P} - {}^lW_\mathcal{K} + 1)/{}^lS_x \rceil, 0\big)\end{aligned}$$

Equation \[eqn:normal\_width\_calc\] calculates output width assuming no
thresholding. But if the output width exceeds the threshold defined by
$\tau$ output width is set to the threshold value as per Equation
\[eqn:new\_width\_calc\_with\_tau\]. Equation \[eqn:new\_in\_width\]
calculates the input width that would produce an output of width
$W^\mathcal{O}_\mathcal{P}$ (think of this as making
$W^{\mathcal{I}}_{\mathcal{P}}$ the subject of equation
\[eqn:normal\_width\_calc\]). If the new input width is smaller than the
original input width, the starting x coordinate should be updated as per
Equation \[eqn:new\_x\_coord\] such that the updated coordinates
correspond to a center crop from the original. Equation
\[eqn:new\_input\_width\] set the input width to the newly calculated
input width and Equation \[eqn:new\_output\_x\] calculates the x
coordinate of the output patch from the updated values.

![(a) Theoretical speedup ratio with projective field thresholding. (b)
Mean Square Error between exact and approximate output of the final
Conv. or Pool
transformation.[]{data-label="fig:proj_thresholding"}](images/proj_thresholding){width="\columnwidth"}

**Theoretical Speedup with Projective Field Thresholding.** We analyze
the theoretical speedup that can be achieved with *projective field
thresholding* approach when a square occlusion patch is placed on the
center of the input image. Figure \[fig:proj\_thresholding\] (a)
presents the results. It can be seen that with increasing $\tau$
attainable theoretical speedup also increases. We also analyze the mean
square error (MSE) between the exact and approximate output of the
activation volume produced by the final Convolution or Pool
transformation with a black occlusion patch placed on the center of the
input image. The results are shown in Figure \[fig:proj\_thresholding\]
(b). With increasing $\tau$ and increasing patch size we see that the
MSE is also increasing.

Approximate: Adaptive Drill-Down {#sec:ada-drill-down}
--------------------------------

*Adaptive drill-down* approach, which is only applicable in the
non-interactive mode, is based on the observation that in many occlusion
based explainability workloads, such as in medical imaging, the regions
of interest will occupy only a small fraction of the entire image. In
such cases, it is unnecessary to inspect the entire image at a higher
resolution with a small stride value for the occlusion patch. In
*adaptive-drill-down* the final occlusion heatmap will be generated
using a two-stage process. At the first stage, a low-resolution heatmap
will be generated by using a larger stride which we call stage one
stride $S_1$. From the heat map generated at stage one, a predefined
drill-down fraction $r_{drill-down}$ of regions with highest probability
drop for the predicted class is identified. At stage two a
high-resolution occlusion map is generated using the original user
provided stride value, also called stage two stride $S_2$, only for the
selected region. A schematic representation of *adaptive drill-down* is
shown in Figure \[fig:adaptive\_drill\_down\] (a).

The amount of speedup that can be obtained from *adaptive drill-down* is
determined by both $r_{drill-down}$ and $S_1$. If the $r_{drill-down}$
is low, only a small region will have to be examined at a higher
resolution and thus it will be faster. However, this smaller region may
not be sufficient to cover all the interesting regions on the image and
hence can result in losing important information. Larger $S_1$ also
reduces the overall runtime as it reduces the time taken for stage one.
But it has the risk of misidentifying interesting regions especially
when the granularity of those regions are smaller than the occlusion
patch size. The speedup obtained by *adaptive drill-down* approach is
equal to the ratio between the number of individual occlusion patch
positions generated for the normal and *adaptive drill-down* approaches.
Number of individual occlusion patch positions generated with a stride
value of $S$ is proportional to $1/S^2$ (total number of patch positions
is equal to
$\frac{H_{\mathcal{I}_{img}}}{S} \times \frac{W_{\mathcal{I}_{img}}}{S}$).
Hence the speedup can be expressed as per Equation
\[eqn:adaptive-drill-down-eqn\]. Figure \[fig:adaptive\_drill\_down\]
(b) conceptually shows how the speedup would vary with $S_1$ when
$r_{drill-down}$ is fixed and vice versa.

$$\begin{aligned}
\label{eqn:adaptive-drill-down-eqn}
\texttt{speedup} = \frac{S^2_1}{S^2_2+r_{drill-down} \times S^2_1}\end{aligned}$$

![(a) Schematic representation of *adaptive drill-down*. (b) Conceptual
diagram showing the effect of $S_1$ and $r_{drill-down}$ on speedup.
[]{data-label="fig:adaptive_drill_down"}](images/adaptive_drill_down){width="\columnwidth"}

System Tuning
-------------

In this section we explain how [<span
style="font-variant:small-caps;">Krypton</span>]{}  sets its internal
configuration parameters for *approximate inference* optimizations.

**Tuning projective field threshold.** The inaccuracies incurred when
applying *projective field thresholding* can cause quality degradation
in the generate approximate heatmap all the way from indistinguishable
changes major structural changes. To measure this quality degradation we
use Structural Similarity (SSIM) Index [@wang2004image] which is one of
the widely used approaches to measuring the *human perceived difference*
between two similar images. When applying SSIM index we treat the
original heatmap as the reference image with no distortions and the
perceived image similarity of the approximate heat map is calculated
with reference to it. The generated SSIM index is a value between $-1$
and $1$, where $1$ corresponds to perfect similarity. Typically SSIM
index values in the range of $0.90-0.95$ are used in practical
applications such as image compression and video encoding as at the
human perception level they produce indistinguishable distortions. For
more details on SSIM Index method, we refer the reader to the original
SSIM Index paper [@wang2004image].

Tuning *projective field threshold* $\tau$ is done during a special
initial tuning phase. During this tuning phase [<span
style="font-variant:small-caps;">Krypton</span>]{}  takes in a sample of
images (default 30) from the operational workload and evaluates SSIM
value of the approximate heatmap (compared to the exact heatmap) for
different $\tau$ values (default values are 1.0, 0.9, 0.8, ..., 0.4).
These $\tau$ versus SSIM data points are then used to fit a
second-degree curve. At the operational time, [<span
style="font-variant:small-caps;">Krypton</span>]{}  requires the user to
provide the expected level of quality for the heatmaps in terms of a
SSIM value. $\tau$ is then selected from the curve fit to match this
target SSIM value. Figure \[fig:system\_tuning\] (a) shows the SSIM
variation and degree two curve fit for different $\tau$ values and three
different CNN models for a tuning set (n=30) from OCT dataset. From the
plots, it can be seen that the distribution of SSIM versus $\tau$ lies
in a lower dimensional manifold and with increasing $\tau$ SSIM also
increases. Figure \[fig:system\_tuning\] (b) shows the cumulative
percentage plots for SSIM deviation for the tune and test sets (n=30)
when the system is tuned for a target SSIM of 0.9. For a target SSIM of
0.9 system picks $\tau$ values of 0.5, 0.7, and 0.9 for VGG16, ResNet18,
and Inception3 models respectively. It can be seen that approximately
more than $50\%$ of test cases will result in an SSIM value of 0.9 or
greater. Even in cases where it performs worse than 0.9 SSIM,
significant ($95\%-100\%$) portion of them are within +0.1 deviation.

**Tuning adaptive drill-down.** As explained in section
\[sec:ada-drill-down\] the speedup obtained by *adaptive drill-down*
approach is determined by two factors; stage one stride value $S_1$ and
drill-down fraction $r_{drill-down}$. For configuring *adaptive
drill-down* [<span style="font-variant:small-caps;">Krypton</span>]{} 
requires the user to provide $r_{drill-down}$ and a target `speedup`
value. $r_{drill-down}$ should be selected based on the user’s
experience and understanding on the relative size of interesting regions
compared to the full image. This is a fair assumption and in most cases,
such as in medical imaging, users will have a fairly good understanding
on the relative size of the interesting regions. However, if the user is
unable to provide this value [<span
style="font-variant:small-caps;">Krypton</span>]{}  will use a default
value of 0.25 as $r_{drill-down}$. The `speedup` value basically
captures user’s input on how much faster the occlusion experiment should
run. Higher speedup values will sacrifice the quality of non-interesting
(1-$r_{drill-down}$) regions for faster execution. The default value for
`speedup` value is three. The way how [<span
style="font-variant:small-caps;">Krypton</span>]{}  configures *adaptive
drill-down* is different to how it configures *projective field
thresholding*. The reason for this is unlike in *projective field
thresholding* in *adaptive drill-down* users have more intuition on the
outcomes of $r_{drill-down}$ and target `speedup` parameters compared to
the SSIM quality value of the final output. Given $r_{drill-down}$,
target `speedup` value, and original occlusion patch stride value $S_2$
(also called stage two stride) [<span
style="font-variant:small-caps;">Krypton</span>]{}  then calculates the
stage one stride value $S_1$ as per equation \[eqn:s1\]. As $S_1$ cannot
be greater than the width $W_{img}$ (similarly height $H_{img}$) of the
image it can be seen that possible values for the `speedup` value are
upper-bounded as per equation \[eqn:speedup\_bound\].

$$\begin{aligned}
\label{eqn:s1}
S_1 = &~ \sqrt{\frac{\texttt{speedup}}{1 - r_{drill-down} \times \texttt{speedup}}} \times S_2\end{aligned}$$

$$\begin{aligned}
\label{eqn:speedup_bound}
\begin{split}
S_1 = \sqrt{\frac{\texttt{speedup}}{1 - r_{drill-down} \times \texttt{speedup}}} \times S_2 < W_{img}\\
\texttt{speedup} < \texttt{min}\Bigg(\frac{W^2_{img}}{S^2_2+r_{drill-down}\times W^2_{img}}, \frac{1}{r_{drill-down}}\Bigg)
\end{split}\end{aligned}$$

![(a) SSIM variation and degree two curve fit for a sample of OCT
dataset. (b) CDF plot for the SSIM deviation for the $\tau$ values
picked from the curve fit for a target SSIM of
0.9.[]{data-label="fig:system_tuning"}](images/system_tuning){width="\columnwidth"}

Experimental Evaluation
=======================

We empirically validate if [<span
style="font-variant:small-caps;">Krypton</span>]{}  is able reduce the
runtime taken for occlusion based deep CNN explainability workloads. We
then conduct controlled experiments to show the individual contribution
of each optimization in [<span
style="font-variant:small-caps;">Krypton</span>]{}  for the overall
system efficiency.

**Datasets.** We use three real-world datasets: *OCT*, *Chest X-Ray*,
and a sample from *ImageNet*. *OCT* has about 84,000 optical coherence
tomography retinal images categorized into four categories: CNV, DME,
DRUSEN, and NORMAL. CNV (choroidal neovascularization), DME (diabetic
macular edema), and DRUSEN are three different varieties of Diabetic
Retinopathy. NORMAL corresponds to healthy retinal images. *Chest X-Ray*
has about 6,000 X-ray images categorized into three categories: VIRAL,
BACTERIAL, and NORMAL. VIRAL and BACTERIAL categories correspond to two
varieties of Pneumonia. NORMAL corresponds to chest X-Rays of healthy
people. Both *OCT* and *Chest X-Ray* datasets are obtained from an
original scientific study [@kermany2018identifying] which uses CNNs for
predicting Diabetic Retinopathy and Pneumonia from radiological images.
*ImageNet* sample dataset contains 1,000 images corresponding to two
hundred categories selected from the original thousand categorical
dataset [@deng2009imagenet].

**Workloads.** We use three popular ImageNet-trained deep CNNs: VGG16
[@vggnet], ResNet18 [@resnet], and Inception3 [@inception], obtained
from [@torchvisionmodels]. They complement each other in terms of model
size, computational cost, amount of theoretical redundancy that exist
for occlusion experiments, and the level of architectural complexity of
the CNN model. For *OCT* and *Chest X-Ray* datasets the three CNN models
are fine-tuned by retraining the final fully-connected layer with
hyper-parameter tuning as per standard practice. More details on the
fine-tuning process are included in the Appendix. Heat map for the
predicted probabilities is generated using Python Matplotlib library’s
`imshow` method using the `jet_r` color scheme. For the heat map,
maximum threshold value is set to `min`$(1, 1.25 \times p)$ and minimum
threshold value is set to $0.75 \times p$ where $p$ is predicted class
probability for the unmodified image. Original images were resized to
the size required by the CNNs ($224\times224$ for VGG16 and ResNet18 and
$299\times299$ for Inception3) and no additional pre-processing is done.
For GPU experiments a batch size of 128 and for CPU experiments a batch
size 16 is used. CPU experiments are executed with a thread parallelism
of 8. All of our datasets, fine-tuning, experiment, and system code will
be made available on our project web page.

**Experimental Setup.** We use a workstation which has 32 GB RAM, Intel
i7-6700 @ 3.40GHz CPU, 1 TB Seagate ST1000DM010-2EP1, and Nvidia Titan X
(Pascal) 12 GB memory GPU. The system runs Ubuntu 16.04 operating system
with PyTorch version of 0.4.0, CUDA version of 9.0, and cuDNN version of
7.1.2. Each runtime reported is the average of three runs with 95%
confidence intervals shown.

End-to-End Evaluation
---------------------

![image](images/5_1_all_edited){width="\textwidth"}

For the GPU based environment we compare two variations [<span
style="font-variant:small-caps;">Krypton</span>]{}, [<span
style="font-variant:small-caps;">Krypton</span>]{}-Exact which only
applies the *incremental inference* optimization and [<span
style="font-variant:small-caps;">Krypton</span>]{}-Approximate which
applies both *incremental inference* and *approximate inference*
optimizations, against two baselines. *Naive* is the current dominant
practice of performing full inference for multiple images with each
corresponding to individual occlusion patch position in batched manner.
*Naive Incremental Inference-Exact* is a pure PyTorch based
implementation of Algorithm \[alg:incinference\] which does not use any
GPU optimized kernels for memory copying where as [<span
style="font-variant:small-caps;">Krypton</span>]{}  does. For CPU based
environments we only compare [<span
style="font-variant:small-caps;">Krypton</span>]{}-Exact and [<span
style="font-variant:small-caps;">Krypton</span>]{}-Approximate against
*Naive* as no customization is needed for the pure PyTorch based
implementation. For different datasets we set *adaptive drill-down*
system tuning parameters differently. For *OCT* images the region of
interest is relative small and hence a $r_{drill-down}$ value of 0.1 and
a target `speedup` of 5 is used. For *Chest X-Ray* images the region of
interest can be large and hence a $r_{drill-down}$ value of 0.4 and a
target `speedup` of 2 is used. For *ImageNet* experiments we use a
$r_{drill-down}$ value of $0.25$ and a target `speedup` value of 3 which
are also the [<span style="font-variant:small-caps;">Krypton</span>]{} 
default values. For all experiments $\tau$ is configured using a
separate tuning image dataset $(n=30)$ for a target SSIM of $0.9$.
Visual examples for each dataset is shown in Appendix. Figure
\[fig:5\_1\_all\_edited\] presents the final results.

We see that [<span style="font-variant:small-caps;">Krypton</span>]{} 
improves the efficiency of the occlusion based explainability workload
across the board. [<span
style="font-variant:small-caps;">Krypton</span>]{}-Approximate for *OCT*
results in the highest speedup with VGG16 on both CPU and GPU
environments (16X for CPU and 34.5X for GPU). Speedups obtained by
[<span style="font-variant:small-caps;">Krypton</span>]{}-Exact for all
the datasets are same for all three CNN models. However, with [<span
style="font-variant:small-caps;">Krypton</span>]{}-Approximate they
result in different speedup values. This is because with *approximate
inference* each dataset uses different system configuration parameters.
*OCT* which is configured with a low $r_{drill-down}$ of 0.1, high
target `speedup` of 5, and a *projective field threshold* value of 0.5
results in the highest speedup. Speedup obtained by [<span
style="font-variant:small-caps;">Krypton</span>]{}-Exact on GPU with
Inception3 model (0.7X) is slightly lower than one. However ResNet18
which has roughly the same theoretical speedup (see Figure
\[fig:redundancy\_ratio\]) results in a higher speedup value (1.6X). The
reason for this is Inception3’s internal architecture is more complex
compared ResNet18 with more branches and depth-wise stacking operations.
Thus Inception3 requires more memory copying operations whose overheads
are not captured by our theoretical speedup calculation. Overall
compared to GPU environment [<span
style="font-variant:small-caps;">Krypton</span>]{}  results in higher
speedups on the CPU environment though the actual runtimes are much
slower. GPUs enable higher parallelism with thousands of processing
cores compared to CPUs with several cores. Hence computations are much
cheaper on GPU. Memory operations required by [<span
style="font-variant:small-caps;">Krypton</span>]{}  throttles the
overall performance on GPU and hinders it from achieving higher
speedups. On CPU environment as computational cost dominates the overall
runtime, the additional overhead introduced by the memory operations
does not matter much. Therefore on CPU [<span
style="font-variant:small-caps;">Krypton</span>]{}  achieves higher
speedups which are closer to the theoretical speedup value. Overall
[<span style="font-variant:small-caps;">Krypton</span>]{}  offers the
best efficiency on these workloads. This confirms the benefits of
different optimizations performed by [<span
style="font-variant:small-caps;">Krypton</span>]{}  for improving the
efficiency of the workload and thereby to reduce the computational and
runtime costs. Bringing down the runtimes also make occlusion
experiments more amenable for interactive diagnosis of CNN predictions.

Lesion Study
------------

We now present the results of controlled experiments that are conducted
to identify the contribution of various optimizations discussed in
Section \[sec:optimizer\]. The speedup values are calculated compared to
the runtime taken by the current dominant practice of performing full
inference for batches of modified images.

**Speedups from Incremental Inference.**

![Theoretical versus empirical speedup for *incremental inference*
(Occlusion patch stride
$S=4$).[]{data-label="fig:5_2_1_edited"}](images/5_2_1_edited){width="\columnwidth"}

We compare theoretical speedup and empirical speedups obtained by
*incremental inference* implementations for both CPU and GPU
environments. The patch sizes that we have selected cover the range of
sizes used in most practical applications. Occlusion patch stride is set
to 4. Figure \[fig:5\_2\_1\_edited\] shows the results. Empirical-GPU
Naive results in the worst performance for all three CNN models.
Empirical-GPU and Empirical-CPU implementations result in higher
speedups with Empirical-CPU being closer to the theoretical speedup
value. As the occlusion patch size increases the speedups decrease.

**Speedups from Projective Field Thresholding.**

![Theoretical versus empirical speedup for *incremental inference* with
*projective field thresholding* (Occlusion patch size = $16 \times 16$,
stride
$S=4$).[]{data-label="fig:5_2_2_edited"}](images/5_2_2_edited){width="\columnwidth"}

We vary *projective field threshold* $\tau$ from 1.0 (no thresholding)
to 0.4 and evaluate the speedups. The occlusion patch size used is 16
and the stride is 4. The results are shown in Figure
\[fig:5\_2\_2\_edited\]. Empirical-CPU and Empirical-GPU both results in
higher speedups with Empirical-CPU being closer to the theoretical
speedup value. When $\tau$ decreases the speedups increase as the amount
of computational savings increase.

**Speedups from Adaptive Drill-Down.**

![Theoretical versus empirical speedup for *adaptive drill-down*
(Occlusion patch size = $16 \times 16$, stage two stride $S_2=4$,
projective field threshold $\tau=1.0$. For (a) $S_1$=16 and for (b)
$r_{drill\_down}$=0.25).[]{data-label="fig:5_2_3_edited"}](images/5_2_3_edited){width="\columnwidth"}

Finally we evaluate the effect of *adaptive drill-down* on overall
[<span style="font-variant:small-caps;">Krypton</span>]{}  efficiency.
The experiments are run on top of the *incremental inference* approach
with no *projective field thresholding* ($\tau$=1.0). $r_{drill-down}$
is varied between 0.1 to 0.5 fixing the stage one stride value $S_1$ to
16. Occlusion patch size is set to 16 and the stage two stride $S_2$ is
set to 4. Figure \[fig:5\_2\_3\_edited\] (a) shows the results. We also
vary $S_1$ fixing $r_{drill-down}$ to 0.25. Occlusion patch size and the
$S_2$ are set same as in the previous case. Figure
\[fig:5\_2\_3\_edited\] (b) presents the results. In both cases we see
Empirical-GPU and Empirical-CPU achieve higher speedups with
Empirical-CPU being very close to the theoretical speedup. On the CPU
environment, the relative cost of other overheads is much smaller than
the CNN computational cost. Hence on the CPU environment [<span
style="font-variant:small-caps;">Krypton</span>]{}  achieves near
theoretical speedups for *adaptive drill-down*. Speedups decrease as we
increase $r_{drill-down}$ and decrease $S_1$.

**Summary of Experimental Results.** Overall [<span
style="font-variant:small-caps;">Krypton</span>]{}  increases the
efficiency of the occlusion based CNN explainability workload by up to
16X on GPU and 34.5X on CPU. Speedup obtained by *approximate inference*
optimization ([<span
style="font-variant:small-caps;">Krypton</span>]{}-Approximate) depends
on the characteristics of the CNN model such as the effective growth of
the projective field and the characteristics of the occlusion use case
such as the relative size of the interesting regions on the image.
Furthermore [<span style="font-variant:small-caps;">Krypton</span>]{} 
results in higher speedups on CPU environment compared to GPU
environment. Increasing the occlusion patch size and $\tau$ decrease the
speedup. Increasing $r_{drill-down}$ and decreasing $S_1$ also decrease
the speedup.

Other Related Work
==================

Conclusions And Future Work
===========================

Special Situations with Incremental Inference
=============================================

![One dimensional representation showing special situations under which
actual output size will be smaller than the values calculated by
Equations \[eqn:xcoordinate\] and \[eqn:ycoordinate\]. (a) and (b) shows
a situation with filter stride being equal to the filter size. (c) and
(d) shows a situation with input patch being placed at the edge of the
input.[]{data-label="fig:less_one_example"}](images/less_one_example){width="\columnwidth"}

It is important to note that there are special situations under which
the actual output patch size can be smaller than the values calculated
in Section \[sec:inc\_computation\]. Consider the simplified one
dimensional situation shown in Figure \[fig:less\_one\_example\] (a),
where the stride value[^2] (3) is same as the filter size (3). In this
situation, the size of the output patch is one less than the value
calculated by Equation \[eqn:patchwidth\]. However, it is not the case
in Figure \[fig:less\_one\_example\] (b) which has the same input patch
size but is placed at a different location. Another situation arises
when the input patch is placed at the edge of the input as shown in
Figure \[fig:less\_one\_example\] (c). In this situation, it is not
possible for the filter to move freely through all filter positions as
it hits the input boundary compared to having the input patch on the
middle of the input as shown in Figure \[fig:less\_one\_example\] (c).
In [<span style="font-variant:small-caps;">Krypton</span>]{}  we do not
treat theses differences separately and use the values calculated by
Equation \[eqn:patchwidth\] and \[eqn:patchheight\] as they act as an
upper bound. In case of a smaller output patch, [<span
style="font-variant:small-caps;">Krypton</span>]{}  simply reads off and
updates slightly bigger patches to preserve uniformity. This also
requires updating the starting coordinates of the patches as shown in
Equations \[eqn:width\_subtract\] and \[eqn:height\_subtract\]. Such
uniform treatment is required for performing batched inference
operations which out of the box gives significant speedups compared to
per image inference.

If
$x^\mathcal{O}_\mathcal{P} + W^\mathcal{O}_\mathcal{P} > W_{\mathcal{O}}:$
$$\begin{aligned}
\begin{split}
\label{eqn:width_subtract}
x^\mathcal{O}_\mathcal{P} = &~ W_{\mathcal{O}} - W^\mathcal{O}_\mathcal{P}\\
x^\mathcal{I}_\mathcal{P} = &~ W_{\mathcal{I}} - W^\mathcal{I}_\mathcal{P}\\
x^\mathcal{R}_\mathcal{P} = &~ W_{\mathcal{I}} - W^\mathcal{R}_\mathcal{P}
\end{split}\end{aligned}$$

If
$y^\mathcal{O}_\mathcal{P} + H^\mathcal{O}_\mathcal{P} > H_{\mathcal{O}}:$
$$\begin{aligned}
\begin{split}
\label{eqn:height_subtract}
y^\mathcal{O}_\mathcal{P} = &~ H_{\mathcal{O}} - H^\mathcal{O}_\mathcal{P}\\
y^\mathcal{I}_\mathcal{P} = &~ H_{\mathcal{I}} - H^\mathcal{I}_\mathcal{P}\\
y^\mathcal{R}_\mathcal{P} = &~ H_{\mathcal{I}} - H^\mathcal{R}_\mathcal{P}
\end{split}\end{aligned}$$

Fine-tuning CNNs
================

For *OCT* and *Chest X-Ray* datasets the three ImageNet pre-trained CNN
models are fine-tuned by retraining the final layer. We use a
train-validation-test split of 60-20-20 and the exact numbers for each
dataset are shown in Table \[tbl:dataset\_sizes\]. Cross-entropy loss
with L2 regularization is used as the loss function and Adam
[@kingma2014adam] is used as the optimizer. We tune learning rate
$\eta \in [10^{-2}, 10^{-4}, 10^{-6}]$ and regularization parameter
$\lambda \in [10^{-2}, 10^{-4}, 10^{-6}]$ using the validation set and
train for 25 epochs. Table \[tbl:finetune\_accuracies\] shows the final
train and test accuracies.

  ------------- -------- -------- ---------
                                  
                                  
  OCT           50,382   16,853   16, 857
  Chest X-Ray   3,463    1,237    1,156
  ------------- -------- -------- ---------

  : Train-validation-test split size for each
  dataset.[]{data-label="tbl:dataset_sizes"}

  -- ------------ ------- ------ -- ----------- --
                                                
                                                
                  Train   Test      $\lambda$   
     VGG16        79      82        $10^{-4}$   
     ResNet18     79      82        $10^{-4}$   
     Inception3   71      81        $10^{-6}$   
     VGG16        75      76        $10^{-4}$   
     ResNet18     78      76        $10^{-6}$   
     Inception3   74      76        $10^{-2}$   
  -- ------------ ------- ------ -- ----------- --

  : Train and test accuracies after
  fine-tuning.[]{data-label="tbl:finetune_accuracies"}

Visual Examples
===============

Figure \[fig:visual\_examples\] presents occlusion heatmaps for a sample
image from each dataset with (a) *incremental inference* and (b)
*incremental inference* with *adaptive drill-down* for different
*projective field threshold* values. The predicted class label for
*OCT*, *Chest X-Ray*, and *ImageNet* are DME, VIRAL, and OBOE
respectively.

![image](images/visual_examples){width="\textwidth"}

[^1]: If the occlusion patch is placed towards a corner of the input
    image the theoretical speedup will be slightly higher. But placing
    the occlusion patch on the center gives us a worst-case estimate.

[^2]: Note that the stride value is generally less than or equal to the
    filter size.
