{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.models.alexnet import alexnet\n",
    "from torchvision.models.vgg import vgg16, vgg19\n",
    "from torchvision.models.resnet import resnet18, resnet50\n",
    "from torchvision.models.densenet import densenet121\n",
    "from torchvision.models.inception import Inception3 as inception3\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import gc\n",
    "from matplotlib import gridspec\n",
    "\n",
    "sys.path.append('../../../code')\n",
    "\n",
    "from python.commons import full_inference_e2e_with_model, inc_inference_e2e_with_model\n",
    "from python.vgg16 import VGG16\n",
    "from python.resnet18 import ResNet18\n",
    "from python.inception3 import Inception3\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_commons import get_ivm_patch_coordinates, calculate_ivm_flops, calculate_flops, mean_confidence_interval\n",
    "\n",
    "random.seed(45)\n",
    "np.random.seed(45)\n",
    "\n",
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_inference_selected_with_model(inc_model, file_path, patch_size, stride, patch_positions, batch_size=64, beta=1.0,\n",
    "                           x0=0, y0=0, image_size=224, x_size=224, y_size=224, gpu=True, version='v1',\n",
    "                           n_labels=1000, weights_data=None, loader=None, c=0.0):\n",
    "\n",
    "    if loader == None:\n",
    "        loader = transforms.Compose([transforms.Resize([image_size, image_size]), transforms.ToTensor()])\n",
    "    orig_image = Image.open(file_path).convert('RGB')\n",
    "    orig_image = loader(orig_image).unsqueeze(0)\n",
    "\n",
    "    if gpu:\n",
    "        orig_image = orig_image.cuda()\n",
    "\n",
    "    image_patches = torch.FloatTensor(3, patch_size, patch_size).fill_(c).repeat(batch_size, 1, 1, 1)\n",
    "        \n",
    "    x_output_width = int(math.ceil((x_size*1.0 - patch_size) / stride))\n",
    "    y_output_width = int(math.ceil((y_size*1.0 - patch_size) / stride))\n",
    "\n",
    "    total_number = x_output_width * y_output_width\n",
    "    logit_values = np.zeros((x_output_width, y_output_width), dtype=np.float32)\n",
    "    \n",
    "    num_batches = int(math.ceil(len(patch_positions) * 1.0 / batch_size))\n",
    "    \n",
    "    if gpu:\n",
    "        inc_model = inc_model.cuda()\n",
    "        \n",
    "    temp = inc_model.forward_materialized(orig_image).cpu().data.numpy()\n",
    "    logit_index = np.argmax(temp)\n",
    "    prob = np.max(temp)\n",
    " \n",
    "    locations = torch.zeros([batch_size, 2], dtype=torch.int32)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        for j in range(batch_size):\n",
    "            index = j * num_batches + i\n",
    "            if index >= len(patch_positions):\n",
    "                break\n",
    "\n",
    "            x, y = patch_positions[index]\n",
    "            x = x*stride + x0\n",
    "            y = y*stride + y0\n",
    "            x,y = int(x), int(y)\n",
    "            \n",
    "            locations[j, 0] = x\n",
    "            locations[j, 1] = y\n",
    "\n",
    "        if version == 'v1':\n",
    "            logits = inc_model.forward_gpu(image_patches, locations, p_height=patch_size, p_width=patch_size)\n",
    "        else:\n",
    "            logits = inc_model.forward_pytorch(image_patches, locations, p_height=patch_size, p_width=patch_size)\n",
    "            \n",
    "        logits = logits.cpu().data.numpy()[:, logit_index].flatten().tolist()\n",
    "\n",
    "        for logit, j in zip(logits, range(batch_size)):\n",
    "            index = j * num_batches + i\n",
    "            if index >= len(patch_positions):\n",
    "                break\n",
    "            x, y = patch_positions[index]\n",
    "            logit_values[x, y] = logit\n",
    "\n",
    "    del inc_model\n",
    "    gc.collect()\n",
    "\n",
    "    return logit_values, prob, logit_index\n",
    "\n",
    "\n",
    "\n",
    "def adaptive_drilldown_with_model(model1, model2, file_path, patch_size, s2, percentile, speedup, batch_size=128, image_size=224,\n",
    "                       x_size=224, y_size=224, beta=1.0, gpu=True, version='v1',\n",
    "                       n_labels=1000, weights_data=None, loader=None, c=0.0):\n",
    "\n",
    "    final_out_width = int(math.ceil((image_size*1.0-patch_size)/s2))\n",
    "    s1 = round(math.sqrt(speedup/(1-percentile*speedup)) * s2)\n",
    "    percentile *= 100 \n",
    "\n",
    "    #checking for interested regions\n",
    "    temp1, prob, logit_index = inc_inference_e2e_with_model(model1, file_path, patch_size, s1,\n",
    "                                    batch_size=batch_size, beta=beta, image_size=image_size, x_size=x_size,\n",
    "                              y_size=y_size, gpu=gpu, version=version, weights_data=weights_data, loader=loader, c=c)\n",
    "\n",
    "    temp1 = cv2.resize(temp1, (final_out_width, final_out_width), interpolation=cv2.INTER_LINEAR)\n",
    "    threshold = np.percentile(temp1, percentile)\n",
    "    temp1 = np.where(temp1 <= threshold, 0, temp1)\n",
    "    idx = np.argwhere(temp1 <= threshold)\n",
    "    \n",
    "    #drilldown into interested regions\n",
    "    temp2, prob, logit_index = inc_inference_selected_with_model(model2, file_path, patch_size, s2, idx.tolist(),\n",
    "                                    batch_size=batch_size, beta=beta, image_size=image_size,\n",
    "                              x_size=image_size, y_size=image_size, gpu=gpu, version=version,\n",
    "                            weights_data=weights_data, loader=loader, c=c)\n",
    "\n",
    "    \n",
    "    \n",
    "    return np.add(temp1, temp2), prob, logit_index\n",
    "\n",
    "def get_r_drill_down_speedup(datset):\n",
    "    if dataset == \"oct\":\n",
    "        return 0.1, 5\n",
    "    elif dataset == \"chest_xray\":\n",
    "        return 0.4, 2\n",
    "    elif dataset == \"imagenet\":\n",
    "        return 0.25, 3\n",
    "    \n",
    "def get_tau(model, dataset):\n",
    "    if model == \"VGG16\":\n",
    "        return 0.5\n",
    "    elif model == \"ResNet18\":\n",
    "        if dataset == \"chest_xray\":\n",
    "            return 0.8\n",
    "        else:\n",
    "            return 0.7\n",
    "    elif model == \"Inception3\":\n",
    "        if dataset == \"chest_xray\":\n",
    "            return 0.8\n",
    "        else:\n",
    "            return 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'orange', 'green', 'red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x108 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\"VGG16\", \"ResNet18\", \"Inception3\"]\n",
    "patch_size = 16\n",
    "S2 = 4\n",
    "c = 0.0\n",
    "runs = 4\n",
    "\n",
    "\n",
    "log_file = open('5_1_GPU.log', 'a')\n",
    "log_file.write('============================================\\n')\n",
    "log_file.flush()\n",
    "\n",
    "datasets = [\"oct\", \"chest_xray\", \"imagenet\"]\n",
    "dataset_names = {\"oct\": \"OCT\", \"chest_xray\": \"Chest X-Ray\", \"imagenet\": \"ImageNet\"}\n",
    "image_file_path = '../../../code/python/dog_resized.jpg'\n",
    "\n",
    "    \n",
    "fig = plt.figure(figsize=(12, 1.5))\n",
    "gs = gridspec.GridSpec(2, 3, height_ratios=[1, 4]) \n",
    "\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    ax1 = plt.subplot(gs[idx])\n",
    "    ax2 = plt.subplot(gs[3+idx])\n",
    "    \n",
    "    \n",
    "    full_inf_mean = []\n",
    "    full_inf_ci = []\n",
    "    ivm_torch_mean = []\n",
    "    ivm_torch_ci = []\n",
    "    ivm_exact_mean = []\n",
    "    ivm_exact_ci = []\n",
    "    ivm_approx_mean = []\n",
    "    ivm_approx_ci = []\n",
    "        \n",
    "        \n",
    "    for model in models:\n",
    "\n",
    "        if model == \"VGG16\":\n",
    "            ivm_model = VGG16\n",
    "            image_size = 224\n",
    "        elif model == \"ResNet18\":\n",
    "            ivm_model = ResNet18\n",
    "            image_size = 224\n",
    "        elif model == \"Inception3\":\n",
    "            ivm_model = Inception3\n",
    "            image_size = 299\n",
    "        \n",
    "        tau = get_tau(model, dataset)\n",
    "\n",
    "        temp_runs = []\n",
    "        cnn_model = ivm_model(gpu=True).eval()\n",
    "        for count in range(runs):\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            prev_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                x, prob, logit_index = full_inference_e2e_with_model(cnn_model, image_file_path, patch_size, S2,\n",
    "                                       batch_size=128, gpu=True, image_size=image_size, x_size=image_size,\n",
    "                                       y_size=image_size, c=c)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            full_inference_time = time.time() - prev_time\n",
    "            temp_runs.append(full_inference_time)\n",
    "            log_file.write(\",\".join([str(x) for x in [count, model, dataset, \"full\", full_inference_time]])+\"\\n\")\n",
    "            log_file.flush()\n",
    "\n",
    "        del cnn_model  \n",
    "        gc.collect()\n",
    "        m, ci = mean_confidence_interval(temp_runs[1:])\n",
    "        full_inf_mean.append(m)\n",
    "        full_inf_ci.append(ci)\n",
    "        \n",
    "        \n",
    "        temp_runs = []\n",
    "        cnn_model = ivm_model(gpu=True, beta=1.0).eval()\n",
    "        for count in range(runs):\n",
    "            \n",
    "            torch.cuda.synchronize()    \n",
    "            prev_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                x, prob, logit_index = inc_inference_e2e_with_model(cnn_model, image_file_path, patch_size, S2,\n",
    "                                      batch_size=128, beta=1.0, gpu=True, version='v2',\n",
    "                                     image_size=image_size, x_size=image_size, y_size=image_size, c=c)\n",
    "            torch.cuda.synchronize()\n",
    "            inc_inference_time = time.time() - prev_time\n",
    "            temp_runs.append(inc_inference_time)\n",
    "            log_file.write(\",\".join([str(x) for x in [count, model, dataset, \"inc-naive\", inc_inference_time]])+\"\\n\")\n",
    "            log_file.flush()\n",
    "          \n",
    "        del cnn_model\n",
    "        gc.collect()\n",
    "        m, ci = mean_confidence_interval(temp_runs[1:])\n",
    "        ivm_torch_mean.append(m)\n",
    "        ivm_torch_ci.append(ci)\n",
    "        \n",
    "        temp_runs = []\n",
    "        cnn_model = ivm_model(gpu=True, beta=1.0).eval()\n",
    "        for count in range(runs):\n",
    "            \n",
    "            torch.cuda.synchronize()    \n",
    "            prev_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                x, prob, logit_index = inc_inference_e2e_with_model(cnn_model, image_file_path, patch_size, S2,\n",
    "                                      batch_size=128, beta=1.0, gpu=True, version='v1',\n",
    "                                     image_size=image_size, x_size=image_size, y_size=image_size, c=c)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            inc_inference_time = time.time() - prev_time\n",
    "            temp_runs.append(inc_inference_time)\n",
    "            log_file.write(\",\".join([str(x) for x in [count, model, dataset, \"inc\", inc_inference_time]])+\"\\n\")\n",
    "            log_file.flush()\n",
    "           \n",
    "        del cnn_model\n",
    "        m, ci = mean_confidence_interval(temp_runs[1:])\n",
    "        ivm_exact_mean.append(m)\n",
    "        ivm_exact_ci.append(ci)\n",
    "        \n",
    "        \n",
    "        temp_runs = []\n",
    "        cnn_model1 = ivm_model(gpu=True, beta=tau).eval()\n",
    "        cnn_model2 = ivm_model(gpu=True, beta=tau).eval()\n",
    "        r_drill, speedup = get_r_drill_down_speedup(dataset)\n",
    "        for count in range(runs):\n",
    "            \n",
    "            torch.cuda.synchronize()    \n",
    "            prev_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                x, prob, logit_index = adaptive_drilldown_with_model(cnn_model1, cnn_model2, image_file_path, patch_size, S2, r_drill, speedup,\n",
    "                                    batch_size=128, beta=tau, gpu=True, version='v1',\n",
    "                                    image_size=image_size, x_size=image_size, y_size=image_size,\n",
    "                                    c=c)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            ada_time = time.time() - prev_time\n",
    "            temp_runs.append(ada_time)\n",
    "            log_file.write(\",\".join([str(x) for x in [count, model, dataset, \"inc-aprox\", ada_time]])+\"\\n\")\n",
    "            log_file.flush()\n",
    "\n",
    "        del cnn_model1\n",
    "        del cnn_model2\n",
    "        gc.collect()\n",
    "        m, ci = mean_confidence_interval(temp_runs[1:])\n",
    "        ivm_approx_mean.append(m)\n",
    "        ivm_approx_ci.append(ci)\n",
    "        \n",
    "        \n",
    "    pos = [0.8, 2., 3.2]\n",
    "    width = 0.2\n",
    "\n",
    "    for ax_temp in [ax1, ax2]:\n",
    "        l1 = ax_temp.bar(pos, full_inf_mean, width, color=colors[0], hatch=3*'-',\n",
    "                     yerr=full_inf_ci, ecolor='black', alpha=0.8) \n",
    "        l2 = ax_temp.bar([p + width for p in pos], ivm_torch_mean, width, color=colors[1], hatch=3*'+',\n",
    "                 yerr=ivm_torch_ci, ecolor='black', alpha=0.8)\n",
    "        l3 = ax_temp.bar([p + 2*width for p in pos], ivm_exact_mean, width, color=colors[2], hatch=3*'x',\n",
    "                 yerr=ivm_exact_ci, ecolor='black', alpha=0.8)\n",
    "        l4 = ax_temp.bar([p + 3*width for p in pos], ivm_approx_mean, width, color=colors[3], hatch=3*'*',\n",
    "                 yerr=ivm_approx_ci, ecolor='black', alpha=0.8)\n",
    "    \n",
    "    \n",
    "        ax_temp.set_xticks([p + 1.5*width for p in pos])\n",
    "        ax_temp.set_xticklabels(models)\n",
    "        ax_temp.set_xlim(min(pos)-width, max(pos)+width*4)\n",
    "    \n",
    "    #if idx == 0:\n",
    "    #    ax.set_ylabel('Run Time (s)')\n",
    "\n",
    "    ax1.set_title(dataset_names[dataset]+\"/GPU\")\n",
    "    ax1.set_ylim(70, 80)\n",
    "    ax2.set_ylim(0, 15)\n",
    "\n",
    "    \n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    \n",
    "    ax1.xaxis.tick_top()\n",
    "    ax1.tick_params(labeltop='off')\n",
    "    ax2.xaxis.tick_bottom()\n",
    "        \n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "   \n",
    "\n",
    "#lgd = fig.legend((l1, l2, l3, l4), ('Naive', 'Naive Inc. Inference-Exact', 'Krypton-Exact', 'Krypton-Approx.'), loc=(0.18, -0.01), ncol=4, frameon=False)\n",
    "#plt.savefig('../images/5_1_GPU.pdf', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.savefig('../images/5_1_GPU.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9, 16.0\n",
      "1.6, 6.2\n",
      "0.7, 4.5\n",
      "3.9, 8.6\n",
      "1.6, 3.1\n",
      "0.7, 2.3\n",
      "3.9, 11.2\n",
      "1.6, 4.4\n",
      "0.7, 2.6\n"
     ]
    }
   ],
   "source": [
    "log_file = open('5_1_GPU.log', 'r')\n",
    "lines = log_file.readlines()[1:]\n",
    "\n",
    "runs = 4\n",
    "count = 0\n",
    "\n",
    "while count < len(lines):\n",
    "    full_inf = []\n",
    "    for r in range(runs):\n",
    "        full_inf.append(float(lines[count].split(\",\")[-1].replace(\"\\n\", \"\")))\n",
    "        count += 1\n",
    "\n",
    "    count += runs\n",
    "    inc_inf = []\n",
    "    for r in range(runs):\n",
    "        inc_inf.append(float(lines[count].split(\",\")[-1].replace(\"\\n\", \"\")))\n",
    "        count += 1\n",
    "    \n",
    "    inc_approx = []\n",
    "    for r in range(runs):\n",
    "        inc_approx.append(float(lines[count].split(\",\")[-1].replace(\"\\n\", \"\")))\n",
    "        count += 1\n",
    "    \n",
    "    m_full, _ = mean_confidence_interval(full_inf[1:])\n",
    "    m_inc, _ = mean_confidence_interval(inc_inf[1:])    \n",
    "    m_approx, _ = mean_confidence_interval(inc_approx[1:]) \n",
    "    \n",
    "    print(\"%.1f, %.1f\"%(m_full/m_inc, m_full/m_approx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
