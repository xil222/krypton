{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.models.alexnet import alexnet\n",
    "from torchvision.models.vgg import vgg16, vgg19\n",
    "from torchvision.models.resnet import resnet18, resnet50\n",
    "from torchvision.models.densenet import densenet121\n",
    "from torchvision.models.inception import Inception3 as inception3\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import gc\n",
    "\n",
    "sys.path.append('../../../code')\n",
    "\n",
    "from python.commons import full_inference_e2e_with_model, inc_inference_e2e_with_model\n",
    "from python.vgg16 import VGG16\n",
    "from python.resnet18 import ResNet18\n",
    "from python.inception3 import Inception3\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_commons import get_ivm_patch_coordinates, calculate_ivm_flops, calculate_flops, mean_confidence_interval\n",
    "\n",
    "random.seed(45)\n",
    "np.random.seed(45)\n",
    "\n",
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_inference_selected_with_model(inc_model, file_path, patch_size, stride, patch_positions, batch_size=64, beta=1.0,\n",
    "                           x0=0, y0=0, image_size=224, x_size=224, y_size=224, gpu=True, version='v1',\n",
    "                           n_labels=1000, weights_data=None, loader=None, c=0.0):\n",
    "\n",
    "    if loader == None:\n",
    "        loader = transforms.Compose([transforms.Resize([image_size, image_size]), transforms.ToTensor()])\n",
    "    orig_image = Image.open(file_path).convert('RGB')\n",
    "    orig_image = loader(orig_image).unsqueeze(0)\n",
    "\n",
    "    if gpu:\n",
    "        orig_image = orig_image.cuda()\n",
    "\n",
    "    image_patches = torch.FloatTensor(3, patch_size, patch_size).fill_(c).repeat(batch_size, 1, 1, 1)\n",
    "        \n",
    "    x_output_width = int(math.ceil((x_size*1.0 - patch_size) / stride))\n",
    "    y_output_width = int(math.ceil((y_size*1.0 - patch_size) / stride))\n",
    "\n",
    "    total_number = x_output_width * y_output_width\n",
    "    logit_values = np.zeros((x_output_width, y_output_width), dtype=np.float32)\n",
    "    \n",
    "    num_batches = int(math.ceil(len(patch_positions) * 1.0 / batch_size))\n",
    "    \n",
    "    if gpu:\n",
    "        inc_model = inc_model.cuda()\n",
    "        \n",
    "    temp = inc_model.forward_materialized(orig_image).cpu().data.numpy()\n",
    "    logit_index = np.argmax(temp)\n",
    "    prob = np.max(temp)\n",
    " \n",
    "    locations = torch.zeros([batch_size, 2], dtype=torch.int32)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        for j in range(batch_size):\n",
    "            index = j * num_batches + i\n",
    "            if index >= len(patch_positions):\n",
    "                break\n",
    "\n",
    "            x, y = patch_positions[index]\n",
    "            x = x*stride + x0\n",
    "            y = y*stride + y0\n",
    "            x,y = int(x), int(y)\n",
    "            \n",
    "            locations[j, 0] = x\n",
    "            locations[j, 1] = y\n",
    "\n",
    "        if version == 'v1':\n",
    "            logits = inc_model.forward_gpu(image_patches, locations, p_height=patch_size, p_width=patch_size)\n",
    "        else:\n",
    "            logits = inc_model.forward_pytorch(image_patches, locations, p_height=patch_size, p_width=patch_size)\n",
    "            \n",
    "        logits = logits.cpu().data.numpy()[:, logit_index].flatten().tolist()\n",
    "\n",
    "        for logit, j in zip(logits, range(batch_size)):\n",
    "            index = j * num_batches + i\n",
    "            if index >= len(patch_positions):\n",
    "                break\n",
    "            x, y = patch_positions[index]\n",
    "            logit_values[x, y] = logit\n",
    "\n",
    "    del inc_model\n",
    "    gc.collect()\n",
    "\n",
    "    return logit_values, prob, logit_index\n",
    "\n",
    "\n",
    "\n",
    "def adaptive_drilldown_with_model(model1, model2, file_path, patch_size, s2, percentile, speedup, batch_size=128, image_size=224,\n",
    "                       x_size=224, y_size=224, beta=1.0, gpu=True, version='v1',\n",
    "                       n_labels=1000, weights_data=None, loader=None, c=0.0):\n",
    "\n",
    "    final_out_width = int(math.ceil((image_size*1.0-patch_size)/s2))\n",
    "    s1 = round(math.sqrt(speedup/(1-percentile*speedup)) * s2)\n",
    "    percentile *= 100 \n",
    "\n",
    "    #checking for interested regions\n",
    "    temp1, prob, logit_index = inc_inference_e2e_with_model(model1, file_path, patch_size, s1,\n",
    "                                    batch_size=batch_size, beta=beta, image_size=image_size, x_size=x_size,\n",
    "                              y_size=y_size, gpu=gpu, version=version, weights_data=weights_data, loader=loader, c=c)\n",
    "\n",
    "    temp1 = cv2.resize(temp1, (final_out_width, final_out_width), interpolation=cv2.INTER_LINEAR)\n",
    "    threshold = np.percentile(temp1, percentile)\n",
    "    temp1 = np.where(temp1 <= threshold, 0, temp1)\n",
    "    idx = np.argwhere(temp1 <= threshold)\n",
    "    \n",
    "    #drilldown into interested regions\n",
    "    temp2, prob, logit_index = inc_inference_selected_with_model(model2, file_path, patch_size, s2, idx.tolist(),\n",
    "                                    batch_size=batch_size, beta=beta, image_size=image_size,\n",
    "                              x_size=image_size, y_size=image_size, gpu=gpu, version=version,\n",
    "                            weights_data=weights_data, loader=loader, c=c)\n",
    "\n",
    "    \n",
    "    \n",
    "    return np.add(temp1, temp2), prob, logit_index\n",
    "\n",
    "def get_r_drill_down_speedup(datset):\n",
    "    if dataset == \"oct\":\n",
    "        return 0.1, 5\n",
    "    elif dataset == \"chest_xray\":\n",
    "        return 0.4, 2\n",
    "    elif dataset == \"imagenet\":\n",
    "        return 0.25, 3\n",
    "    \n",
    "def get_tau(model, dataset):\n",
    "    if model == \"VGG16\":\n",
    "        return 0.5\n",
    "    elif model == \"ResNet18\":\n",
    "        if dataset == \"chest_xray\":\n",
    "            return 0.8\n",
    "        else:\n",
    "            return 0.7\n",
    "    elif model == \"Inception3\":\n",
    "        if dataset == \"chest_xray\":\n",
    "            return 0.8\n",
    "        else:\n",
    "            return 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'orange', 'green', 'red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = [\"VGG16\", \"ResNet18\", \"Inception3\"]\n",
    "patch_size = 16\n",
    "S2 = 4\n",
    "c = 0.0\n",
    "runs = 3\n",
    "\n",
    "datasets = [\"oct\", \"chest_xray\", \"imagenet\"]\n",
    "dataset_names = {\"oct\": \"OCT\", \"chest_xray\": \"Chest X-Ray\", \"imagenet\": \"ImageNet\"}\n",
    "image_file_path = '../../../code/python/dog_resized.jpg'\n",
    "\n",
    "log_file = open('5_1_CPU.log', 'a')\n",
    "log_file.write(\"=========================================================\\n\")\n",
    "log_file.flush()\n",
    "    \n",
    "fig = plt.figure(figsize=(12, 2.5))    \n",
    "for idx, dataset in enumerate(datasets):\n",
    "    ax = plt.subplot(1, 3, idx+1)\n",
    "    \n",
    "    full_inf_mean = []\n",
    "    full_inf_ci = []\n",
    "    ivm_exact_mean = []\n",
    "    ivm_exact_ci = []\n",
    "    ivm_approx_mean = []\n",
    "    ivm_approx_ci = []\n",
    "           \n",
    "    for model in models:\n",
    "\n",
    "        if model == \"VGG16\":\n",
    "            ivm_model = VGG16\n",
    "            image_size = 224\n",
    "        elif model == \"ResNet18\":\n",
    "            ivm_model = ResNet18\n",
    "            image_size = 224\n",
    "        elif model == \"Inception3\":\n",
    "            ivm_model = Inception3\n",
    "            image_size = 299\n",
    "\n",
    "        tau = get_tau(model, dataset)\n",
    "\n",
    "        temp_runs = []\n",
    "#         cnn_model = ivm_model(gpu=False).eval()\n",
    "#         for count in range(runs):\n",
    "\n",
    "#             torch.cuda.synchronize()\n",
    "#             prev_time = time.time()\n",
    "#             with torch.no_grad():\n",
    "#                 x, prob, logit_index = full_inference_e2e_with_model(cnn_model, image_file_path, patch_size, S2,\n",
    "#                                        batch_size=16, gpu=False, image_size=image_size, x_size=image_size,\n",
    "#                                        y_size=image_size, c=c)\n",
    "\n",
    "#             torch.cuda.synchronize()\n",
    "#             full_inference_time = time.time() - prev_time\n",
    "#             log_file.write(\",\".join([str(x) for x in [dataset, model, count, \"full-inf\", full_inference_time]])+\"\\n\")\n",
    "#             log_file.flush()\n",
    "#             temp_runs.append(full_inference_time)\n",
    "\n",
    "#         del cnn_model\n",
    "#         gc.collect()\n",
    "#         m, ci = mean_confidence_interval(temp_runs)\n",
    "#         full_inf_mean.append(m)\n",
    "#         full_inf_ci.append(ci)\n",
    "\n",
    "\n",
    "#         temp_runs = []\n",
    "#         cnn_model = ivm_model(gpu=False).eval()\n",
    "#         for count in range(runs):\n",
    "\n",
    "#             torch.cuda.synchronize()    \n",
    "#             prev_time = time.time()\n",
    "#             with torch.no_grad():\n",
    "#                 x, prob, logit_index = inc_inference_e2e_with_model(cnn_model, image_file_path, patch_size, S2,\n",
    "#                                       batch_size=16, beta=1.0, gpu=False, version='v2',\n",
    "#                                      image_size=image_size, x_size=image_size, y_size=image_size, c=c)\n",
    "\n",
    "#             torch.cuda.synchronize()\n",
    "#             inc_inference_time = time.time() - prev_time\n",
    "#             log_file.write(\",\".join([str(x) for x in [dataset, model, count, \"inc-inf\", inc_inference_time]])+\"\\n\")\n",
    "#             log_file.flush()            \n",
    "#             temp_runs.append(inc_inference_time)\n",
    "\n",
    "#         del cnn_model\n",
    "#         gc.collect()\n",
    "#         m, ci = mean_confidence_interval(temp_runs)\n",
    "#         ivm_exact_mean.append(m)\n",
    "#         ivm_exact_ci.append(ci)\n",
    "\n",
    "\n",
    "        temp_runs = []\n",
    "        cnn_model1 = ivm_model(gpu=False, beta=tau).eval()\n",
    "        cnn_model2 = ivm_model(gpu=False, beta=tau).eval()\n",
    "        r_drill, speedup = get_r_drill_down_speedup(dataset)\n",
    "        for count in range(runs):\n",
    "\n",
    "            torch.cuda.synchronize()    \n",
    "            prev_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                x, prob, logit_index = adaptive_drilldown_with_model(cnn_model1, cnn_model2, image_file_path, patch_size, S2, r_drill, speedup,\n",
    "                                    batch_size=16, beta=tau, gpu=False, version='v2',\n",
    "                                    image_size=image_size, x_size=image_size, y_size=image_size,\n",
    "                                    c=c)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            ada_time = time.time() - prev_time\n",
    "            log_file.write(\",\".join([str(x) for x in [dataset, model, count, \"inc-inf-approx\", ada_time]])+\"\\n\")\n",
    "            log_file.flush()            \n",
    "            temp_runs.append(ada_time)\n",
    "\n",
    "        del cnn_model1\n",
    "        del cnn_model2\n",
    "        gc.collect()\n",
    "        m, ci = mean_confidence_interval(temp_runs)\n",
    "        ivm_approx_mean.append(m)\n",
    "        ivm_approx_ci.append(ci)\n",
    "\n",
    "\n",
    "#     pos = [0.8, 2., 3.2]\n",
    "#     width = 0.2\n",
    "\n",
    "#     l1 = plt.bar(pos, full_inf_mean, width, color=colors[0], hatch=3*'-',\n",
    "#                  yerr=full_inf_ci, ecolor='black', alpha=0.8) \n",
    "#     l3 = plt.bar([p + width for p in pos], ivm_exact_mean, width, color=colors[2], hatch=3*'x',\n",
    "#              yerr=ivm_exact_ci, ecolor='black', alpha=0.8)\n",
    "#     l4 = plt.bar([p + 2*width for p in pos], ivm_approx_mean, width, color=colors[3], hatch=3*'*',\n",
    "#              yerr=ivm_approx_ci, ecolor='black', alpha=0.8)\n",
    "    \n",
    "#     #ax.set_ylabel('Run Time (s)')\n",
    "    \n",
    "#     ax.set_xticks([p + width for p in pos])\n",
    "#     ax.set_title(dataset_names[dataset]+\"/CPU\")\n",
    "#     ax.set_xticklabels(models)\n",
    "#     plt.xlim(min(pos)-width, max(pos)+width*3)\n",
    "#     plt.grid()\n",
    "        \n",
    "   \n",
    "\n",
    "# lgd = fig.legend((l1, l3, l4), ('Naive', 'Krypton-Exact', 'Krypton-Approx.'), loc=(0.25, -0.01), ncol=3, frameon=False)\n",
    "\n",
    "# plt.savefig('../images/5_1_CPU.pdf', bbox_extra_artists=(lgd,), bbox_inches='tight')   \n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x108 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\"VGG16\", \"ResNet18\", \"Inception3\"]\n",
    "patch_size = 16\n",
    "S2 = 4\n",
    "c = 0.0\n",
    "runs = 3\n",
    "\n",
    "datasets = [\"oct\", \"chest_xray\", \"imagenet\"]\n",
    "dataset_names = {\"oct\": \"OCT\", \"chest_xray\": \"Chest X-Ray\", \"imagenet\": \"ImageNet\"}\n",
    "image_file_path = '../../../code/python/dog_resized.jpg'\n",
    "\n",
    "log_file = open('5_1_CPU.log', 'r')\n",
    "\n",
    "lines = log_file.readlines()[1:]\n",
    "count = 0\n",
    "    \n",
    "    \n",
    "fig = plt.figure(figsize=(12, 1.5))    \n",
    "for idx, dataset in enumerate(datasets):\n",
    "    ax = plt.subplot(1, 3, idx+1)\n",
    "    \n",
    "    full_inf_mean = []\n",
    "    full_inf_ci = []\n",
    "    ivm_exact_mean = []\n",
    "    ivm_exact_ci = []\n",
    "    ivm_approx_mean = []\n",
    "    ivm_approx_ci = []\n",
    "           \n",
    "    for model in models:\n",
    "\n",
    "        if model == \"VGG16\":\n",
    "            ivm_model = VGG16\n",
    "            image_size = 224\n",
    "        elif model == \"ResNet18\":\n",
    "            ivm_model = ResNet18\n",
    "            image_size = 224\n",
    "        elif model == \"Inception3\":\n",
    "            ivm_model = Inception3\n",
    "            image_size = 299\n",
    "\n",
    "        tau = get_tau(model, dataset)\n",
    "\n",
    "        temp_runs = []\n",
    "        for r in range(runs):\n",
    "            temp_runs.append(float(lines[count].split(\",\")[-1].replace(\"\\n\",\"\")))\n",
    "            count += 1\n",
    "        \n",
    "        m, ci = mean_confidence_interval(temp_runs)\n",
    "        full_inf_mean.append(m)\n",
    "        full_inf_ci.append(ci)\n",
    "\n",
    "\n",
    "        temp_runs = []\n",
    "        for r in range(runs):\n",
    "            temp_runs.append(float(lines[count].split(\",\")[-1].replace(\"\\n\",\"\")))\n",
    "            count += 1\n",
    "\n",
    "        m, ci = mean_confidence_interval(temp_runs)\n",
    "        ivm_exact_mean.append(m)\n",
    "        ivm_exact_ci.append(ci)\n",
    "\n",
    "\n",
    "        temp_runs = []\n",
    "        for r in range(runs):\n",
    "            temp_runs.append(float(lines[count].split(\",\")[-1].replace(\"\\n\",\"\")))\n",
    "            count += 1\n",
    "\n",
    "        m, ci = mean_confidence_interval(temp_runs)\n",
    "        ivm_approx_mean.append(m)\n",
    "        ivm_approx_ci.append(ci)\n",
    "\n",
    "\n",
    "    pos = [0.8, 2., 3.2]\n",
    "    width = 0.2\n",
    "\n",
    "    l1 = plt.bar(pos, full_inf_mean, width, color=colors[0], hatch=3*'-',\n",
    "                 yerr=full_inf_ci, ecolor='black', alpha=0.8) \n",
    "    l3 = plt.bar([p + width for p in pos], ivm_exact_mean, width, color=colors[2], hatch=3*'x',\n",
    "             yerr=ivm_exact_ci, ecolor='black', alpha=0.8)\n",
    "    l4 = plt.bar([p + 2*width for p in pos], ivm_approx_mean, width, color=colors[3], hatch=3*'*',\n",
    "             yerr=ivm_approx_ci, ecolor='black', alpha=0.8)\n",
    "    \n",
    "    #ax.set_ylabel('Run Time (s)')\n",
    "    \n",
    "    ax.set_xticks([p + width for p in pos])\n",
    "    ax.set_title(dataset_names[dataset]+\"/CPU\")\n",
    "    ax.set_xticklabels(models)\n",
    "    plt.xlim(min(pos)-width, max(pos)+width*3)\n",
    "    plt.grid()\n",
    "        \n",
    "   \n",
    "\n",
    "#lgd = fig.legend((l1, l3, l4), ('Naive', 'Krypton-Exact', 'Krypton-Approx.'), loc=(0.32, -0.04), ncol=3, frameon=False)\n",
    "\n",
    "plt.savefig('../images/5_1_CPU.pdf', bbox_extra_artists=(), bbox_inches='tight')   \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.4, 34.5\n",
      "2.1, 14.8\n",
      "1.5, 8.0\n",
      "5.4, 13.8\n",
      "2.1, 4.9\n",
      "1.5, 3.7\n",
      "5.4, 19.9\n",
      "2.1, 8.5\n",
      "1.5, 4.7\n"
     ]
    }
   ],
   "source": [
    "log_file = open('5_1_CPU.log', 'r')\n",
    "lines = log_file.readlines()[1:]\n",
    "\n",
    "runs = 3\n",
    "count = 0\n",
    "\n",
    "while count < len(lines):\n",
    "    full_inf = []\n",
    "    for r in range(runs):\n",
    "        full_inf.append(float(lines[count].split(\",\")[-1].replace(\"\\n\", \"\")))\n",
    "        count += 1\n",
    "\n",
    "    inc_inf = []\n",
    "    for r in range(runs):\n",
    "        inc_inf.append(float(lines[count].split(\",\")[-1].replace(\"\\n\", \"\")))\n",
    "        count += 1\n",
    "    \n",
    "    inc_approx = []\n",
    "    for r in range(runs):\n",
    "        inc_approx.append(float(lines[count].split(\",\")[-1].replace(\"\\n\", \"\")))\n",
    "        count += 1\n",
    "    \n",
    "    m_full, _ = mean_confidence_interval(full_inf)\n",
    "    m_inc, _ = mean_confidence_interval(inc_inf)    \n",
    "    m_approx, _ = mean_confidence_interval(inc_approx) \n",
    "    \n",
    "    print(\"%.1f, %.1f\"%(m_full/m_inc, m_full/m_approx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
