%!TEX root = <main.tex>
\section{Preliminaries and Overview}\label{sec:preliminaries}
In this section, we first formally state the problem and explain our assumptions.
Then we formalize the internals of critical layers in a deep CNN for the purpose of proposing our \textit{incremental inference} approach in Section 4.
\eat{
Finally we briefly explain the Structural Similarity Index (SSIM) which is used to quantify the quality of the generated sensitivity heat maps.
}

\subsection{Problem Statement and Assumptions}\label{sec:problem}

\begin{table}[t]
  \centering
  \caption{Symbols used in the Section 3}
  \scalebox{0.8}{\begin{tabular}{p{2cm}p{7.5cm}}
    \toprule
    \textbf{Symbol} & \textbf{Meaning}\\
    \midrule \midrule
    $f$ & Fine-tuned CNN which takes in an input image and outputs a probability distribution over the class labels\\
    \midrule
    $T_{:l}$ & Tensor transformation function used in the $l^{th}$ layer of the CNN $f$\\
    \midrule
    $L$ & Class label predicted by $f$ for the original image $\mathcal{I}_{:img}$\\
    \midrule
    $\mathcal{P}$ & Occluding patch in RGB format\\
    \midrule
    $S_\mathcal{P}$ & Occluding patch striding amount\\
    \midrule
    $G$ & Set of occluding patch superimposition positions on $\mathcal{I}_{:img}$ in (x,y) format\\
    \midrule
    $M$ & Heat map produced by the occlusion experiment\\
    \midrule
    $H_M,W_M$ & Height and width of $M$\\
    \midrule
    $\bm\circ_{x,y}$ & Superimposition operator. $A~\circ_{x,y}~B$, superimposes $B$ on top of $A$ starting off at $(x,y)$ position\\
    \midrule
    $\mathcal{I}_{:l} (\mathcal{I}_{:img})$ & Input tensor of the $l^{th}$ layer (Input Image)\\
    \midrule
    $\mathcal{O}_{:l}$ & Output tensor of the $l^{th}$ layer\\
    \midrule
    $C_{\mathcal{I}:l},H_{\mathcal{I}:l},W_{\mathcal{I}:l}$ & Depth, height, and width of $l^th$ layer Input\\
    \midrule
    $C_{\mathcal{O}:l},H_{\mathcal{O}:l},W_{\mathcal{O}:l}$ & Depth, height, and width of $l^{th}$ layer Output\\
    \midrule
    $\mathcal{K}_{conv:l}$ & Convolution filter kernels for the $l^{th}$ layer\\
    \midrule
    $\mathcal{B}_{conv:l}$ & Convolution bias value vector for the $l^{th}$ layer\\
    \midrule
    $\mathcal{K}_{pool:l}$ & Pooling filter kernel for the $l^{th}$ layer\\
    \midrule
    $H_{\mathcal{K}:l},W_{\mathcal{K}:l}$ & Height and width of the filter kernel for the $l^{th}$ layer\\
    \midrule
    $S_{:l}$$\equiv$$(S_{x:l},S_{y:l})$ & Filter kernel patch striding amount for the $l^{th}$ layer ($S_{x:l}$ and $S_{y:l}$ corresponds to width and height dimensions)\\
    \midrule
    $P_{:l}$$\equiv$$(P_{x:l},P_{y:l})$ & Padding amount for the $l^{th}$ layer ($P_{x:l}$ and $P_{y:l}$ corresponds to padding along width and height dimensions)\\
    % $Q (Q_{inc})$ & Total FLOPS count with full (incremental) inference\\
    % \midrule
    % $L$ & Set of convolution layers in a CNN\\
    % \midrule
    \bottomrule
  \end{tabular}}
\label{table:preliminaries_symbols}
\end{table}

We are given a CNN $f$ which consists of a sequence or a DAG of tensor transformation functions $T_{:l}$s, an image $\mathcal{I}_{:img}$ on which the occlusion experiment needs to be run, the predicted class label $L$ for $\mathcal{I}_{:img}$, an occluding patch $\mathcal{P}$ in RGB format, and occluding patch striding amount $S_{\mathcal{P}}$.
We are also given a set of interested occluding patch positions $G$, constructed either automatically or by the human with a visual interface interactively.
The occlusion experiment workload is to generate a 2-D heat map $M$, where each value correspond to the coordinates in $G$ contains the predicted probability for $L$ by $f$ for the occluded image $\mathcal{I}^{'}_{x,y:img}$ or zero otherwise.
More precisely, we can state the workload using the following set of logical statements:

\begin{align}
\label{eqn:mheight}
W_M =&~ \lfloor(\texttt{width}(\mathcal{I}_{:img}) - \texttt{width}(\mathcal{P}) + 1)/S_\mathcal{P}\rfloor\\
\label{eqn:mwidth}
H_M =&~ \lfloor(\texttt{height}(\mathcal{I}_{:img}) - \texttt{height}(\mathcal{P}) + 1)/S_\mathcal{P}\rfloor\\
M \in&~ \mathcal{\rm I\!R}^{H_M \times W_M}\\
\forall~ x,y \in&~ G:\\
\label{eqn:patchimpose}
& \mathcal{I}^{'}_{x,y:img} \leftarrow \mathcal{I}_{:img} ~ \bm\circ_{x,y} ~ \mathcal{P} \\
\label{eqn:outputval}
& M[x,y] \leftarrow f(\mathcal{I}^{'}_{x,y:img})[L]
\end{align}


Step (\ref{eqn:mheight}), and (\ref{eqn:mwidth}) calculates the dimensions of the generated heat map $M$ which is dependent on the dimensions of $\mathcal{I}_{:img}$, $\mathcal{P}$, and $S_\mathcal{P}$.
Step (\ref{eqn:patchimpose}) superimposes $\mathcal{P}$ on $\mathcal{I}_{:img}$ with its top left corner placed on the (x,y) location of $\mathcal{I}_{:img}$.
Step (\ref{eqn:outputval}) calculates the output value at the (x,y) location by performing CNN inference for $\mathcal{I}^{'}_{x,y:img}$ using $f$ and picking the predicted probability for the label $L$.
Step (\ref{eqn:patchimpose}) and (\ref{eqn:outputval}) are run for all occluding patch position values in $G$.
In the non-interactive mode $G$ is initialized to $G = [0, H_M) \times [0, W_M)$, which corresponds to the set of all possible occlusion patch positions on  $\mathcal{I}_{:img}$.
In the interactive mode it is possible that human operator would provide multiple $G$s, one after the other, for which the system has to evaluate iteratively.

The focus of this work is on the occlusion based deep CNN explainability approach \cite{zeiler2014visualizing}, which is widely used by practitioners in several domains including healthcare, sociology, security, and agriculture \cite{kermany2018identifying, islam2017abnormality, mohanty2016using, arbabzadah2016identifying, wang2017deep}.
However, we acknowledge that there are other methods for CNN explainability and a summary of those approaches is included in the Appendix.
We assume that $f$ is a CNN from a roster of well-known CNNs (currently, we support VGG 16-layer version, ResNet 18-layer version, and Inception version 3).
We think this is a reasonable start, since most recent CNN-based image recognition applications use only such well-known CNNs from model zoos \cite{caffemodelzoo, tfmodelzoo}.
Nevertheless, our work is orthogonal to the specifics of a particular CNN architecture, and our proposed techniques can be easily extended to any CNN architecture.
We leave support for arbitrary CNNs to future work.

\subsection{Deep CNN Internals}\label{sec:cnn_internals}
Deep CNNs are a type of neural networks specialized for image data.
They are organized into multiple layers of various types including: \textit{Convolution}, which use image filters from graphics, except with variable filter weights, to extract features; \textit{Pooling}, which subsamples features in a spatial locality-aware way; \textit{Batch-Normalization}, which normalizes the output of the layer; \textit{Non-Linearity}, which applies a non-linear transformation (e.g., ReLU); \textit{Fully-Connected} which is an ordered collection of perceptrons.
Input and output of individual layers in a deep CNN, except for Fully-Connected ones, are arranged into 3-D tensors that have a width, height, and depth.
For example an RGB input image of 224$\times$224 dimensions can be considered as an input tensor having a width and height of 224 and a depth of 3.
Every non-Fully-Connected layer will take in an input tensor and transform it into another tensor.
A Fully-Connected layer takes in a 1-D tensor or a flattened 3-D tensor as input and transforms it into another 1-D tensor.
For our purpose, these transformations can be broadly divided into three categories based on their spatial locality:

\begin{figure*}[t]
\includegraphics[width=\textwidth]{images/cnn_simplified}
\caption{Simplified representation of selected layers of a Deep CNN. The values marked in red show how a small spatial update in the first input would propagate through subsequent layers. (a) Convolution layer (for simplicity addition of bias is not shown in the Convolution transformation), (b) ReLU layer, and (c) Pool layer. Notation is explained in Table ~\ref{table:preliminaries_symbols}.}
\label{fig:cnn_simplified}
\end{figure*}

\begin{itemize}
    \item Transformations that operate at the granularity of a global context.
    \begin{itemize}
     \item E.g. Fully-Connected
    \end{itemize}
	\item Transformations that operate at the granularity of individual  spatial locations.
	\begin{itemize}
	 \item E.g. ReLU, Batch Normalization
	\end{itemize}
	\item Transformations that operate at the granularity of a local spatial context.
	\begin{itemize}
	 \item E.g. Convolution, Pooling
	\end{itemize}
\end{itemize}

\vspace{2mm}
\noindent \textbf{Transformations that operate at the granularity of a global context.} These transformations operate on a global context and does not take into account the spatial information.
Fully-Connected, which is the only global transformation layer in a CNN, takes in a 1-D tensor or a flattened 3-D tensor and performs a matrix-vector multiplication with a weight matrix and produces an output 1-D tensor.
Since it performs one bulk transformation, there is almost no substantial opportunity for exploiting redundancies with incremental computations.
The computational cost of a Full-Connected transformation is proportional to the product of the size of the input and output 1-D tensors.
Fully-Connected layers are used as the last or last few layers in a CNN and only accounts for a small fraction of the total computational cost.
Thus we ignore them henceforth.

\vspace{2mm}
\noindent \textbf{Transformations that operate at the granularity of individual  spatial locations.} These transformations essentially perform a $map(.)$ function on each individual element in the tensor (see Figure \ref{fig:cnn_simplified} (b)).
Hence, the output will have the same dimensions as input.
The computational cost incurred by these transformations is proportional to the volume of the input (or output).
However, with incremental spatially localized updates in the input, such as placing an occlusion patch, only the updated region needs to be recalculated.
Extending these transformations to become change aware is straightforward since they are element-wise operations.
The computational cost of the change aware incremental transformation is proportional to the volume of only the modified region.


\vspace{2mm}
\noindent \textbf{Transformations that operate at the granularity of a local spatial context.}
With incremental spatially localized updates in the input, transformations that operate at the granularity of a local spatial context also provide opportunities for exploiting redundancy and can be made change aware.
However, with local context transformations, such as Convolution and Pooling, this extension is non-trivial due to the overlapping nature of the spatial contexts.
% Convolution layers are the most important type of layer in the CNN architecture which also contributes to most of the computational cost.

Each Convolution layer can have $C_{\mathcal{O}:l}$ 3-D ``filter kernels'' organized into a 4-D array $\mathcal{K}_{conv:l}$, with each having a smaller spatial width $W_{\mathcal{K}:l}$ and height $H_{\mathcal{K}:l}$ compared to the width $W_{\mathcal{I}:l}$ and height $H_{\mathcal{I}:l}$ of the input tensor $\mathcal{I}_{:l}$, but has the same depth $C_{\mathcal{I}:l}$.
During inference, $c^{th}$ filter kernel is ``strided'' along the width and height dimensions of the input and a 2-D ``activation map'' $A_{:c}=(a_{y,x:c})\in \mathcal{\rm I\!R}^{H_{\mathcal{O}:l} \times~ W_{\mathcal{O}:l}}$ is produced by calculating element-wise products between the kernel and the input and adding a bias value as per Equation (\ref{eqn:elementwise_product}).
The computational cost of each of these individual element-wise products is proportional to the volume of the filter kernel.
Finally, these 2-D activation maps are stacked together along the depth dimension to produce an output tensor $\mathcal{O}_{:l} \in \mathcal{\rm I\!R}^{C_{\mathcal{O}:l} \times H_{\mathcal{O}:l} \times W_{\mathcal{O}:l}}$ as per Equation (\ref{eqn:conv_operator}).
A simplified representation of Convolution transformation is shown in Figure \ref{fig:cnn_simplified} (a).


\begin{align}
\label{eqn:elementwise_product}
\begin{split}
a_{y,x:c} =& \sum_{k=0}^{C_{\mathcal{I}:l} } \sum_{j=0}^{H_{\mathcal{K}:l}-1} \sum_{i=0}^{W_{\mathcal{K}:l}-1} \mathcal{K}_{conv:l}[c, k, j, i] \\
& \quad \times \mathcal{I}_{:l}[k,y-\floor{\frac{H_{\mathcal{K}:l} }{2}}+j,x-\floor{\frac{W_{\mathcal{K}:l} }{2}}+i] \\
& \quad + \mathcal{B}_{conv:l}[c]
\end{split}
\end{align}

\begin{align}
\label{eqn:conv_operator}
\begin{split}
\mathcal{O}_{:l} = [A_{:0}, A_{:1}, ... , A_{(C_{\mathcal{O}:l}-1)}]
\end{split}
\end{align}


% \eat{
% \begin{align}
% \begin{split}
% \text{Input Volume}:&~ \mathcal{I} \in \mathcal{\rm I\!R}^{C_{\mathcal{I}} \times H_{\mathcal{I}} \times W_{\mathcal{I}}}\\
% \text{Convolution Filters}:&~ \mathcal{K}_{conv} \in \mathcal{\rm I\!R}^{C_{\mathcal{O}} \times C_{\mathcal{I}} \times H_{\mathcal{K}} \times W_{\mathcal{K}}}\\
% \text{Convolution Bias Vector}:&~ \mathcal{B}_{conv} \in \mathcal{\rm I\!R}^{C_{\mathcal{O}}}\\
% \text{Output Volume}:&~ \mathcal{O} \in \mathcal{\rm I\!R}^{C_{\mathcal{O}} \times H_{\mathcal{O}} \times W_{\mathcal{O}}}
% \end{split}
% \end{align}

% \begin{equation}
% \label{eqn:conv_operator}
% \begin{split}
% \mathcal{O}[c,y,x] &= \sum_{k=0}^{C_{\mathcal{I}}} \sum_{j=0}^{H_\mathcal{K}-1} \sum_{i=0}^{W_\mathcal{K}-1} \mathcal{K}_{conv}[c, k, j, i] \\ & \quad \quad \quad \times \mathcal{I}[k,y-\floor{\frac{H_\mathcal{K}}{2}}+j,x-\floor{\frac{W_\mathcal{K}}{2}}+i] + \mathcal{B}[c]
% \end{split}
% \end{equation}
% }

Pooling can also be thought of as a Convolution operation with a fixed (i.e., not learned) 2-D filter kernel $\mathcal{K}_{pool:l}$.
But unlike Convolution, Pooling operates independently on each depth slice of the input tensor.
% The two main variations of pooling layers are max pooling (takes the maximum value from the local spatial context) and average (takes the average value from the local spatial context) pooling.
A Pooling layer takes a 3-D activation tensor $\mathcal{O}_{l}$ having a depth of $C_{\mathcal{I}:l}$, width of $W_{\mathcal{I}:l}$, and height of $H_{\mathcal{I}:l}$ as input and produces another 3-D activation tensor $\mathcal{O}_{:l}$ with the same depth of $C_{\mathcal{O}:l}=C_{\mathcal{I}:l}$, width of $W_{\mathcal{O}:l}$, and height of $H_{\mathcal{O}:l}$ as the output.
Pooling kernel is generally strided with more than one pixel at a time and hence $W_{\mathcal{O}:l}$ and $H_{\mathcal{O}:l}$ are generally smaller than $W_{\mathcal{I}:l}$ and $H_{\mathcal{I}:l}$.
A simplified representation of Pooling transformation is shown in Figure \ref{fig:cnn_simplified} (c).
% Similar to Convolution, Pooling operation can be formally defined as follows:

% \begin{align}
% \text{Pool Filters}:&~ \mathcal{K}_{pool} \in \mathcal{\rm I\!R}^{H_{\mathcal{K}} \times W_{\mathcal{K}}}
% \end{align}

% \begin{equation}
% \label{eqn:pool_operator}
% \begin{split}
% \mathcal{O}[c,y,x] &= \sum_{j=0}^{H_\mathcal{K}-1} \sum_{i=0}^{W_\mathcal{K}-1} \mathcal{K}_{pool}[j, i] \\ & \quad \quad \quad \times \mathcal{I}[c,y-\floor{\frac{H_\mathcal{K}}{2}}+j,x-\floor{\frac{W_\mathcal{K}}{2}}+i]
% \end{split}
% \end{equation}

It can be seen that Convolution and Pooling transformations can be cast into a form of applying a filter along the spatial dimensions of the 3-D input tensor.
However, how each transformation operates along the depth dimension is different.
For our purpose, since we are only interested in finding the spatial propagation of the patches in the input through the consecutive layers, both of these transformations can be treated similarly.

\vspace{2mm}
\noindent \textbf{Relationship between Input and Output Dimensions.}
The output tensor's dimensions $W_{\mathcal{O}:l}$ and $H_{\mathcal{O}:l}$ are determined by the dimensions of the input tensor $W_{\mathcal{I}:l}$ and $H_{\mathcal{I}:l}$, dimensions of the filter kernel $W_{\mathcal{K}:l}$ and $H_{\mathcal{K}:l}$ and two other parameters: \textbf{stride} $S_{:l}$ and \textbf{padding} $P_{:l}$.
Stride is the number of pixel values used to move the filter kernel at a time when producing a 2-D activation map.
It is possible to have two different values, with one for the width dimension $S_{x:l}$ and one for the height dimension $S_{y:l}$.
Generally $S_{x:l} \leq W_{\mathcal{K}:l}$ and $S_{y:l} \leq H_{\mathcal{K}:l}$.
In Figure \ref{fig:cnn_simplified}, Convolution transformation uses a stride value of 1 and Pool transformation uses a stride value of 2 for both dimensions.
Sometimes, in order to control the dimensions of the output tensor to be same as the input tensor, one needs to pad the input tensor with zeros around the spatial border.
Padding $P_{:l}$ captures the number of zeros that needs to be added.
Similar to the stride $S_{:l}$, it is possible to have two separate values for padding, with one for the width dimension $P_{x:l}$ and one for the height dimension $P_{y:l}$.
In Figure \ref{fig:cnn_simplified}, Convolution transformation pads the input with one line of zeros on both dimensions.
With these parameters defined, the width (similarly height) of the output tensor can be defined as follows:

\begin{align}
\begin{split}
W_{\mathcal{O}:l} = (W_{\mathcal{I}:l} - W_{\mathcal{K}:l} + 1 + 2\times P_{x:l})/S_{x:l} \\
% ^lH_{\mathcal{O}} = (^lH_{\mathcal{I}} - ^lH_\mathcal{K} + 1 + 2\times ^lP_y)/^lS_y
\end{split}
\end{align}

\vspace{2mm}
\noindent \textbf{Computational Cost of Deep CNNs.}
Deep CNNs are highly compute-intensive.
Of all the types of layers, Convolution layers almost always contribute to $90\%$ (or more) of the computation.
Hence, we can roughly estimate the computational cost of a Deep CNN by counting the number of fused multiply-add (FMA) floating point operations (FLOPs) required by Convolution layers for a single forward pass for inference.
% and ignore the computational cost of other layers (e.g. Pooling, Fully-Connected).

For example, applying a Convolution filter having the dimensions of ($C_{\mathcal{I}:l}$, $H_{\mathcal{K}:l}$, $W_{\mathcal{K}:l}$) to compute a single value in the output tensor will require $C_{\mathcal{I}:l} \times H_{\mathcal{K}:l} \times W_{\mathcal{K}:l}$ FLOPs, each corresponding to a single element-wise multiplication.
Thus, the total amount of computations $Q_{:l}$ required by that layer in order to produce an output tensor $\mathcal{O}_{:l}$ with dimensions $C_{\mathcal{O}:l} \times H_{\mathcal{O}:l} \times lW_{\mathcal{O}:l}$, and the total amount of computations $Q$ required to process the entire set of convolution layers $L$ in the CNN can be calculated as per Equation (\ref{eqn:full_local}) and (\ref{eqn:full_all}).

\begin{align}
\label{eqn:full_local}
Q_{:l} =&~ (C_{\mathcal{I}:l} \times H_{\mathcal{K}:l} \times W_{\mathcal{K}:l}) \times (C_{\mathcal{O}:l} \times H_{\mathcal{O}:l} \times W_{\mathcal{O}:l})\\
\label{eqn:full_all}
Q =&~ \sum_{l \in L}Q_{:l}
\end{align}


\eat{
\subsection{Estimating the Quality of Generated Approximate Heat Maps}

When applying approximate inference optimizations \system~ sacrifices the the accuracy/quality of the generated heat map in favor of faster execution.
To measure this drop of accuracy we use Structural Similarity (SSIM) Index~\cite{wang2004image} which is one of the widely used approaches to measure the \textit{human perceived difference} between two similar images.
When applying SSIM index we treat the original heat map as the reference image with no distortions and the perceived image similarity of the \system~generated heat map is calculated with reference to it.
The generated SSIM index is a value between $-1$ and $1$, where $1$ corresponds to perfect similarity.
% It is important to note that, even though SSIM index value of 1 corresponds to perfect similarity, other values doesn't necessarily imply same level of perceived quality across different image pairs.
% However, if the original images are closely similar, such as in chest X-ray images, it can be assumed that this condition will hold.
Typically SSIM index values in the range of $0.90-0.95$ are used in practical applications such as image compression and video encoding as at the human perception level they produce indistinguishable distortions.
For more details on SSIM Index method, we refer the reader to the original SSIM Index paper~\cite{wang2004image}.
}

