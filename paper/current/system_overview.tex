%!TEX root = <main.tex>
\section{Setup and Preliminaries}\label{sec:preliminaries}
We now state our problem formally, introduce our notation, and explain our assumptions.
We then formalize the dataflow of the transformations inside a CNN, since these lay the foundations for understanding our proposed optimizations in Sections 3 and 4.
Table~\ref{table:preliminaries_symbols} lists our notation.
\eat{
Finally we briefly explain the Structural Similarity Index (SSIM) which is used to quantify the quality of the generated sensitivity heat maps.
}

\subsection{Problem Statement and Assumptions}\label{sec:problem}

\begin{table}[t]
  \centering
  \caption{Notation used in this paper.}
  \scalebox{0.8}{\begin{tabular}{p{2cm}p{7.5cm}}
    \toprule
    \textbf{Symbol} & \textbf{Meaning}\\
    \midrule \midrule
    $f$ & Given deep CNN; input is an image tensor; output is a probability distribution over class labels\\
    \midrule
    $L$ & Class label predicted by $f$ for the original image $\mathcal{I}_{:img}$\\
    \midrule
    $T_{:l}$ & Tensor transformation function of layer $l$ of the given CNN $f$\\
    \midrule
    $\mathcal{P}$ & Occluding patch in RGB format\\
    \midrule
    $S_\mathcal{P}$ & Occluding patch striding amount\\
    \midrule
    $G$ & Set of occluding patch superimposition positions on $\mathcal{I}_{:img}$ in (x,y) format\\
    \midrule
    $M$ & Heat map produced by the OBE workload\\
    \midrule
    $H_M,W_M$ & Height and width of $M$\\
    \midrule
    $\bm\circ_{(x,y)}$ & Superimposition operator. $A~\circ_{(x,y)}~B$, superimposes $B$ on top of $A$ starting at $(x,y)$ position\\
    \midrule
    $\mathcal{I}_{:l} ~(\mathcal{I}_{:img})$ & Input tensor of layer $l$ (Input Image)\\
    \midrule
    $\mathcal{O}_{:l}$ & Output tensor of layer $l$\\
    \midrule
    $C_{\mathcal{I}:l},H_{\mathcal{I}:l},W_{\mathcal{I}:l}$ & Depth, height, and width of input of layer $l$\\
    \midrule
    $C_{\mathcal{O}:l},H_{\mathcal{O}:l},W_{\mathcal{O}:l}$ & Depth, height, and width of output of layer $l$\\
    \midrule
    $\mathcal{K}_{conv:l}$ & Convolution filter kernels of layer $l$\\
    \midrule
    $\mathcal{B}_{conv:l}$ & Convolution bias value vector of layer $l$\\
    \midrule
    $\mathcal{K}_{pool:l}$ & Pooling filter kernel of layer $l$\\
    \midrule
    $H_{\mathcal{K}:l},W_{\mathcal{K}:l}$ & Height and width of filter kernel of layer $l$\\
    \midrule
    $S_{:l}; S_{x:l}; S_{y:l}$ & Filter kernel striding amounts of layer $l$; $S_{:l} \equiv (S_{x:l}, S_{y:l})$, strides along width and height dimensions\\
    \midrule
    $P_{:l}; (P_{x:l}; P_{y:l}$ & Padding amounts of layer $l$; $P_{:l} \equiv (P_{x:l}, P_{y:l})$, padding along width and height dimensions\\
    % $Q (Q_{inc})$ & Total FLOPS count with full (incremental) inference\\
    % \midrule
    % $L$ & Set of convolution layers in a CNN\\
    % \midrule
    \bottomrule
  \end{tabular}}
\label{table:preliminaries_symbols}
\end{table}

We are given a CNN $f$ that consists of a sequence (or DAG) of \textit{tensor transformation functions} $T_{:l}$, the image $\mathcal{I}_{:img}$ for which the occlusion-based explanation is needed, the predicted class label $L$ on $\mathcal{I}_{:img}$, an occluding patch $\mathcal{P}$ in RGB format, and occluding patch \textit{stride} $S_{\mathcal{P}}$. We are also given a set of occluding patch positions $G$ constructed either automatically or manually with a visual interface interactively.
The occlusion-based explanation (OBE) workload is as follows: produce a 2-D heat map $M$, wherein each value corresponding to the coordinates in $G$ has the prediction probability of $L$ by $f$ on the occluded image $\mathcal{I}^{'}_{x,y:img}$ (or zero otherwise).
More precisely, the OBE workload can be described with the following logical statements:

\vspace{-2mm}
\begin{align}
\label{eqn:mheight}
W_M =&~ \lfloor(\texttt{width}(\mathcal{I}_{:img}) - \texttt{width}(\mathcal{P}) + 1)/S_\mathcal{P}\rfloor\\
\label{eqn:mwidth}
H_M =&~ \lfloor(\texttt{height}(\mathcal{I}_{:img}) - \texttt{height}(\mathcal{P}) + 1)/S_\mathcal{P}\rfloor\\
M \in&~ \mathcal{\rm I\!R}^{H_M \times W_M}\\
\forall~ (x,y) \in&~ G:\\
\label{eqn:patchimpose}
& \mathcal{I}^{'}_{x,y:img} \leftarrow \mathcal{I}_{:img} ~ \bm\circ_{(x,y)} ~ \mathcal{P} \\
\label{eqn:outputval}
& M[x,y] \leftarrow f(\mathcal{I}^{'}_{x,y:img})[L]
\end{align}

Steps (\ref{eqn:mheight}) and (\ref{eqn:mwidth}) calculate the dimensions of the heat map $M$ produced, which are dependent on the dimensions of $\mathcal{I}_{:img}$, $\mathcal{P}$, and $S_\mathcal{P}$.
Step (\ref{eqn:patchimpose}) superimposes $\mathcal{P}$ on $\mathcal{I}_{:img}$ with its top left corner placed on the $(x,y)$ location of $\mathcal{I}_{:img}$.
Step (\ref{eqn:outputval}) calculates the output value at the $(x,y)$ location by performing CNN inference for $\mathcal{I}^{'}_{x,y:img}$ using $f$ and picks the prediction probability of $L$.
Steps (\ref{eqn:patchimpose}) and (\ref{eqn:outputval}) are performed for each occluding patch position value in $G$ \textit{independently}.
In the non-interactive mode, $G$ is initialized to $G = [0, H_M) \times [0, W_M)$. Intuitively, this represents the set of all possible occlusion patch positions on $\mathcal{I}_{:img}$. In the interactive mode, the user provides the patch positions manually, which could be a sequence of sets at a time.

\eat{%TODO: This part should go up to the introduction
The focus of this work is on the occlusion based deep CNN explainability approach \cite{zeiler2014visualizing}, which is widely used by practitioners in several domains including healthcare, sociology, security, and agriculture \cite{kermany2018identifying, islam2017abnormality, mohanty2016using, arbabzadah2016identifying, wang2017deep}.
}
We focus on CNN applications with a discrete set of labels for prediction, since only such applications typically use OBE. One could create CNNs that directly predict an image segmentation instead, but labeling image segments is highly tedious and expensive. Thus, many recent applications of CNNs in healthcare, sociology, security, and other domains rely on discrete labels and use OBE~\cite{kermany2018identifying, islam2017abnormality, mohanty2016using, arbabzadah2016identifying, wang2017deep}. Alternative approaches to explain CNN predictions have been studied in the literature; since these are orthogonal to our focus, we summarize them in the appendix due to space constraints.
We assume that $f$ is from a roster of well-known deep CNNs; we currently support VGG-16, ResNet-18, and Inception V3. We think this is a reasonable start, since most recent applications of deep CNNs in our target domains use only such well-known CNNs from model zoos~\cite{caffemodelzoo, tfmodelzoo}. Nevertheless, our techniques are generic across all CNNs, and it is straightforward to extend our ideas to any CNN architecture; we leave support for arbitrary CNNs to future work.

\subsection{Deep CNN Internals}\label{sec:cnn_internals}
CNNs are neural networks specialized for image data. They are organized into many layers of various types, each of which transforms a tensor (a multidimensional array, typically 3-D) into another tensor: \textit{Convolution} uses image filters from graphics, except with variable filter weights, to extract features; \textit{Pooling} subsamples features in a spatial locality-aware way; \textit{Batch-Normalization} normalizes the output of that layer; \textit{Non-Linearity} applies an element-wise non-linear transformation (e.g., ReLU); \textit{Fully-Connected} is an ordered collection of perceptrons~\cite{dlbook}. The output tensor of a layer can have a different width, height, and/or depth than the input tensor. The input to the CNN is an image, which can be seen as a tensor; e.g., a 224$\times$224 RGB image is a tensor of width and height 224 and a depth of 3. A Fully-Connected layer converts a 1-D tensor (or a ``flattened'' 3-D tensor) into another 1-D tensor. To simplify the exposition of our ideas, we now group CNN layers into three main categories based on the \textit{spatial locality} of how their transform a tensor:

\begin{figure*}[t]
\includegraphics[width=\textwidth]{images/cnn_simplified}
\caption{Simplified illustration of the key layers of a typical CNN. The highlighted cells (dark/red background) show how a small local spatial context in the first input propagates through subsequent layers. (a) Convolution layer (for simplicity sake, bias addition is not shown). (b) ReLU Non-linearity layer. (c) Pooling layer (max pooling). Notation is explained in Table~\ref{table:preliminaries_symbols}.}
\label{fig:cnn_simplified}
\end{figure*}

\begin{itemize}
    \item Transformations at the granularity of a \textit{global context}, e.g., Fully-Connected
    
	\item Transformations at the granularity of \textit{individual elements}, e.g., ReLU, Batch Normalization

	\item Transformations at the granularity of a \textit{local spatial context}, e.g. Convolution, Pooling
\end{itemize}

\vspace{2mm}
\noindent \textbf{Transformations at the granularity of a global context.} 
Such layers convert the input tensor holistically into an output tensor without any spatial context for the computations.
Fully-Connected is the only layer of this type in CNNs. It performs a bulk matrix-vector multiplication.
Since every element of the output is potentially affected by the entire input, such layers do not have any major opportunity for exploiting redundancies with incremental computations during re-inference. Thankfully, Fully-Connected layers typically arise only as the last layer (or last few layers) in deep CNNs, and they typically account for a negligible fraction of the total computational cost. Thus, we do not focus further on such layers for our optimizations.
%The computational cost of a Full-Connected transformation is proportional to the product of the size of the input and output 1-D tensors.

\vspace{2mm}
\noindent \textbf{Transformations at the granularity of individual elements.} 
Such layers essentially perform a ``map()'' function on the elements of the input tensor, as illustrated in Figure~\ref{fig:cnn_simplified}(b).
Thus, the output tensor has the same dimensions as the input tensor.
The computational cost of such transformations is proportional to the ``volume'' of the input (product of the dimensions).
If the input is incrementally modified during OBE by placing an occlusion patch, only the exact corresponding region of the output needs to be recomputed during re-inference, since the other parts of the output do not change. Thus, performing incremental inference for such layers is relatively straightforward.
The computational cost of the incremental computation is proportional to the volume of the modified region.


\vspace{2mm}
\noindent \textbf{Transformations at the granularity of a local spatial context.}
Such layers essentially perform weighted aggregations of various slices of the input tensor, called the \textit{local spatial contexts}, by multiplying each with a \textit{filter kernel} (a tensor of weight parameters learned during CNN training). This means the input and output tensors can have different width, height, and/or depth.
Thus, if the input is incrementally modified during OBE by placing an occlusion patch, the region of the output that gets modified is not straightforward to ascertain--this requires non-trivial and careful calculations due to the overlapping nature of how filters get multiplied with the spatial contexts. 
Both Convolution and Pooling come under this category; since such layers typically account for the bulk of the computational cost of inference with a deep CNN, it is crucial to deeply understand how to modify these layers to enable incremental inference. The rest of this section precisely explains the dataflow of such layers' computations using our notation. Section 3 then uses this machinery to dive into our incremental inference optimization for such layers.
% Convolution layers are the most important type of layer in the CNN architecture which also contributes to most of the computational cost.

\vspace{2mm}
\noindent \textbf{Dataflow of Convolution Layers.} 
A layer $l$ has $C_{\mathcal{O}:l}$ 3-D filter kernels arranged as a 4-D array $\mathcal{K}_{conv:l}$, with each having a smaller spatial width $W_{\mathcal{K}:l}$ and height $H_{\mathcal{K}:l}$ than the width $W_{\mathcal{I}:l}$ and height $H_{\mathcal{I}:l}$ of the input tensor $\mathcal{I}_{:l}$ but the same depth $C_{\mathcal{I}:l}$. During inference, $c^{th}$ filter kernel is ``strided'' along the width and height dimensions of the input to produce a 2-D ``activation map'' $A_{:c}=(a_{y,x:c})\in \mathcal{\rm I\!R}^{H_{\mathcal{O}:l} \times~ W_{\mathcal{O}:l}}$ by computing element-wise products between the kernel and the local spatial context and adding a bias value as per Equation (\ref{eqn:elementwise_product}). The computational cost of these small matrix products is proportional to the volume of the filter kernel. All the 2-D activation maps are then stacked along the depth dimension to produce the output tensor $\mathcal{O}_{:l} \in \mathcal{\rm I\!R}^{C_{\mathcal{O}:l} \times H_{\mathcal{O}:l} \times W_{\mathcal{O}:l}}$ as per Equation (\ref{eqn:conv_operator}). Figure~\ref{fig:cnn_simplified}(a) presents a simplified illustration of a Convolution layer's transformation.


\begin{align}
\label{eqn:elementwise_product}
\begin{split}
a_{y,x:c} =& \sum_{k=0}^{C_{\mathcal{I}:l} } \sum_{j=0}^{H_{\mathcal{K}:l}-1} \sum_{i=0}^{W_{\mathcal{K}:l}-1} \mathcal{K}_{conv:l}[c, k, j, i] \\
& \quad \times \mathcal{I}_{:l}[k,y-\floor{\frac{H_{\mathcal{K}:l} }{2}}+j,x-\floor{\frac{W_{\mathcal{K}:l} }{2}}+i] \\
& \quad + \mathcal{B}_{conv:l}[c]
\end{split}
\end{align}

\begin{align}
\label{eqn:conv_operator}
\begin{split}
\mathcal{O}_{:l} = [A_{:0}, A_{:1}, ... , A_{(C_{\mathcal{O}:l}-1)}]
\end{split}
\end{align}


% \eat{
% \begin{align}
% \begin{split}
% \text{Input Volume}:&~ \mathcal{I} \in \mathcal{\rm I\!R}^{C_{\mathcal{I}} \times H_{\mathcal{I}} \times W_{\mathcal{I}}}\\
% \text{Convolution Filters}:&~ \mathcal{K}_{conv} \in \mathcal{\rm I\!R}^{C_{\mathcal{O}} \times C_{\mathcal{I}} \times H_{\mathcal{K}} \times W_{\mathcal{K}}}\\
% \text{Convolution Bias Vector}:&~ \mathcal{B}_{conv} \in \mathcal{\rm I\!R}^{C_{\mathcal{O}}}\\
% \text{Output Volume}:&~ \mathcal{O} \in \mathcal{\rm I\!R}^{C_{\mathcal{O}} \times H_{\mathcal{O}} \times W_{\mathcal{O}}}
% \end{split}
% \end{align}

% \begin{equation}
% \label{eqn:conv_operator}
% \begin{split}
% \mathcal{O}[c,y,x] &= \sum_{k=0}^{C_{\mathcal{I}}} \sum_{j=0}^{H_\mathcal{K}-1} \sum_{i=0}^{W_\mathcal{K}-1} \mathcal{K}_{conv}[c, k, j, i] \\ & \quad \quad \quad \times \mathcal{I}[k,y-\floor{\frac{H_\mathcal{K}}{2}}+j,x-\floor{\frac{W_\mathcal{K}}{2}}+i] + \mathcal{B}[c]
% \end{split}
% \end{equation}
% }

\vspace{2mm}
\noindent \textbf{Dataflow of Pooling Layers.} 
A Pooling layer behaves essentially like a Convolution layer but with a fixed (i.e., not learned) 2-D filter kernel $\mathcal{K}_{pool:l}$. Such kernels typically aggregate the local spatial context to compute the maximum element (``max pooling'') or average (``average pooling'').
But unlike Convolution, Pooling operates independently on each depth-wise slice of the input tensor.
% The two main variations of pooling layers are max pooling (takes the maximum value from the local spatial context) and average (takes the average value from the local spatial context) pooling.
It takes as input a 3-D tensor $\mathcal{O}_{l}$ of depth $C_{\mathcal{I}:l}$, width $W_{\mathcal{I}:l}$, and height $H_{\mathcal{I}:l}$ as input and produces as output a 3-D tensor $\mathcal{O}_{:l}$ with the same depth $C_{\mathcal{O}:l}=C_{\mathcal{I}:l}$ but a different width of $W_{\mathcal{O}:l}$ and height $H_{\mathcal{O}:l}$. In a Pooling layer, the filter kernel is generally strided more than one pixel at a time. Thus, $W_{\mathcal{O}:l}$ and $H_{\mathcal{O}:l}$ are usually smaller than $W_{\mathcal{I}:l}$ and $H_{\mathcal{I}:l}$. Figure~\ref{fig:cnn_simplified}(c) presents a simplified illustration of a Pooling layer's transformation.
Overall, both Convolution and Pooling layers have a similar dataflow: they apply a filter along the width and height dimensions of the input tensor but differ on how they operate along the depth dimension. We note that the OBE workload only modifies the spatial contexts of the tensors (width and height dimensions) across consecutive layers. This lets us handle both Convolution and Pooling layers in a unified manner for the purpose of our incremental inference optimization.

% Similar to Convolution, Pooling operation can be formally defined as follows:

% \begin{align}
% \text{Pool Filters}:&~ \mathcal{K}_{pool} \in \mathcal{\rm I\!R}^{H_{\mathcal{K}} \times W_{\mathcal{K}}}
% \end{align}

% \begin{equation}
% \label{eqn:pool_operator}
% \begin{split}
% \mathcal{O}[c,y,x] &= \sum_{j=0}^{H_\mathcal{K}-1} \sum_{i=0}^{W_\mathcal{K}-1} \mathcal{K}_{pool}[j, i] \\ & \quad \quad \quad \times \mathcal{I}[c,y-\floor{\frac{H_\mathcal{K}}{2}}+j,x-\floor{\frac{W_\mathcal{K}}{2}}+i]
% \end{split}
% \end{equation}

\vspace{2mm}
\noindent \textbf{Relationship between Input and Output Dimensions.}
For Convolution and Pooling layers, $W_{\mathcal{O}:l}$ and $H_{\mathcal{O}:l}$ are determined solely by $W_{\mathcal{I}:l}$ and $H_{\mathcal{I}:l}$, $W_{\mathcal{K}:l}$ and $H_{\mathcal{K}:l}$, and two other parameters that are specific to that layer: \textbf{stride} $S_{:l}$ and \textbf{padding} $P_{:l}$. Stride is the number of pixel values by which the filter kernel is moved at a time. One can have different strides along the width and height dimensions: $S_{x:l}$ and $S_{y:l}$, respectively, but in practice, almost all CNNs have $S_{x:l} = S_{y:l}$. Typically, $S_{x:l} \leq W_{\mathcal{K}:l}$ and $S_{y:l} \leq H_{\mathcal{K}:l}$. In Figure~\ref{fig:cnn_simplified}, the Convolution layer has $S_{x:l} = S_{y:l} = 1$, while the Pooling layer has $S_{x:l} = S_{y:l} = 2$. For some layers, in order to control the dimensions of the output tensor to be same as the input tensor, one needs to ``pad'' the input tensor with zeros along the width and height dimensions. \textit{Padding} $P_{:l}$ captures the number of zeros that need to be placed. It is possible to have two separate padding values along the width and height dimensions ($P_{x:l}$ and $P_{y:l}$). In Figure~\ref{fig:cnn_simplified}(a), the Convolution layer pads the input with $P_{x:l} = P_{y:l} = 1$ line of zeros. Given all these parameters, the width (similarly height) of the output tensor is given by the following formula:

\vspace{-4mm}
\begin{align}
\begin{split}
W_{\mathcal{O}:l} = (W_{\mathcal{I}:l} - W_{\mathcal{K}:l} + 1 + 2\times P_{x:l})/S_{x:l} \\
% ^lH_{\mathcal{O}} = (^lH_{\mathcal{I}} - ^lH_\mathcal{K} + 1 + 2\times ^lP_y)/^lS_y
\end{split}
\end{align}

\vspace{2mm}
\noindent \textbf{Computational Costs of CNN Layers.}
Deep CNN inference is highly computationally expensive, with the Convolution layers typically accounting for a bulk of the cost ($90\%$ or more)~\cite{cnnprofile}. We can roughly estimate the computational cost of a deep CNN by counting the number of \textit{fused multiply-add} (FMA) floating point operations (FLOPs) required by Convolution layers for one inference pass.
% and ignore the computational cost of other layers (e.g. Pooling, Fully-Connected).
For example, applying a Convolution filter with dimensions $(C_{\mathcal{I}:l} , H_{\mathcal{K}:l} , W_{\mathcal{K}:l})$ to compute one element of the output tensor requires $C_{\mathcal{I}:l}  H_{\mathcal{K}:l}  W_{\mathcal{K}:l}$ FLOPs, with each FLOP corresponding to one element-wise multiplication. Thus, the total computational cost $Q_{:l}$ of a layer that produces output $\mathcal{O}_{:l}$ of dimensions $(C_{\mathcal{O}:l} , H_{\mathcal{O}:l} , W_{\mathcal{O}:l})$ and the total computational cost $Q$ of processing the entire set of Convolution layers $L$ of a given CNN can be estimated as per Equations~(\ref{eqn:full_local}) and~(\ref{eqn:full_all}).

\vspace{-4mm}
\begin{align}
\label{eqn:full_local}
Q_{:l} =&~ (C_{\mathcal{I}:l}  H_{\mathcal{K}:l}  W_{\mathcal{K}:l})  (C_{\mathcal{O}:l}  H_{\mathcal{O}:l}  W_{\mathcal{O}:l})\\
\label{eqn:full_all}
Q =&~ \sum_{l \in L}Q_{:l}
\end{align}


\eat{
\subsection{Estimating the Quality of Generated Approximate Heat Maps}

When applying approximate inference optimizations \system~ sacrifices the the accuracy/quality of the generated heat map in favor of faster execution.
To measure this drop of accuracy we use Structural Similarity (SSIM) Index~\cite{wang2004image} which is one of the widely used approaches to measure the \textit{human perceived difference} between two similar images.
When applying SSIM index we treat the original heat map as the reference image with no distortions and the perceived image similarity of the \system~generated heat map is calculated with reference to it.
The generated SSIM index is a value between $-1$ and $1$, where $1$ corresponds to perfect similarity.
% It is important to note that, even though SSIM index value of 1 corresponds to perfect similarity, other values doesn't necessarily imply same level of perceived quality across different image pairs.
% However, if the original images are closely similar, such as in chest X-ray images, it can be assumed that this condition will hold.
Typically SSIM index values in the range of $0.90-0.95$ are used in practical applications such as image compression and video encoding as at the human perception level they produce indistinguishable distortions.
For more details on SSIM Index method, we refer the reader to the original SSIM Index paper~\cite{wang2004image}.
}

